{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newfrogg/data_engineering/blob/sql_intro/data_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Content\n"
      ],
      "metadata": {
        "id": "jWwQ7Tnvp6Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Table of Content](#scrollTo=jWwQ7Tnvp6Se)\n",
        "\n",
        ">[What is ELT ?](#scrollTo=Cf76ae0IeEcl)\n",
        "\n",
        ">>[Data Engineering for short](#scrollTo=2xn5RpTpae9U)\n",
        "\n",
        ">>[What is Data Engineering](#scrollTo=Gvwe2gkoTv8K)\n",
        "\n",
        ">>[What is ETL ?](#scrollTo=RBSCyNIkeHxL)\n",
        "\n",
        ">>>[Example: Amazon Web scraping](#scrollTo=6W_OdARtpRMl)\n",
        "\n",
        ">[Review Python](#scrollTo=cR6dB4EQ_NYh)\n",
        "\n",
        ">>[Project Overview](#scrollTo=qLmqhyMrNBu1)\n",
        "\n",
        ">>[Week 1: Python Basics and Data Ingestion](#scrollTo=LMYJy-QAWQa7)\n",
        "\n",
        ">>>[Day 1: Setup and Python Basics](#scrollTo=ppD7QXboNHV8)\n",
        "\n",
        ">>>[Day 2: Lists and Basic Data Structures](#scrollTo=VTxwG1VqTMsk)\n",
        "\n",
        ">>>[Day 3: Conditionals and Data Cleaning](#scrollTo=c20qkDHXOp2E)\n",
        "\n",
        ">>>>[Why does it print None, None?](#scrollTo=Uv5MH40NUdsN)\n",
        "\n",
        ">>>>[What is a Side Effect?](#scrollTo=JPRSPIw5V3Ik)\n",
        "\n",
        ">>>[Day 4: Loops and Data Aggregation](#scrollTo=UH9ZvdNgWKvC)\n",
        "\n",
        ">>>>>[An alternative using pandas library](#scrollTo=ehSNeEIWbj6V)\n",
        "\n",
        ">>>[Day 5: Functions and Modularity](#scrollTo=gz3Y_nQkdLfF)\n",
        "\n",
        ">>[Week 2: Tuples, Analysis, and Pipeline Completion](#scrollTo=I1iH51nDejBt)\n",
        "\n",
        ">>>[Day 6: Tuples and Immutable Data](#scrollTo=9v22PN1Uen2I)\n",
        "\n",
        ">>>[Day 7: Data Analysis with Lists and Tuples](#scrollTo=0945dymciGFc)\n",
        "\n",
        ">>>>>[Difference Between lambda and def Keyword](#scrollTo=Vgj2B3UsnhhJ)\n",
        "\n",
        ">>>>>>[Usage with filter/map function()](#scrollTo=cRMfFLgGuxB_)\n",
        "\n",
        ">>>>>>[Others: usage with reduce function()](#scrollTo=DglGibyuvC0G)\n",
        "\n",
        ">>>[Day 8: Error Handling and Robustness](#scrollTo=1NmHU7IWv3qW)\n",
        "\n",
        ">>>[Day 9: Final Pipeline Integration](#scrollTo=LQmWOushl_p_)\n",
        "\n",
        ">>>[Day 10: Testing and Extensions](#scrollTo=7VtSWXzXnIA3)\n",
        "\n",
        ">>[Achievement & Insights](#scrollTo=xkoHpXV-_ZuQ)\n",
        "\n",
        ">>>[Deliverables](#scrollTo=xkoHpXV-_ZuQ)\n",
        "\n",
        ">>>[Learning Outcomes](#scrollTo=xkoHpXV-_ZuQ)\n",
        "\n",
        ">>>[Next Steps](#scrollTo=xkoHpXV-_ZuQ)\n",
        "\n",
        ">[SQL Basics (deprecated)](#scrollTo=sf6lN9_4ltQu)\n",
        "\n",
        ">[CS50 SQL](#scrollTo=IBATgsrIQ-a5)\n",
        "\n",
        ">>[What is Database ?](#scrollTo=Zuj-d_npaTCy)\n",
        "\n",
        ">>>[SQL](#scrollTo=8DYc0-GebPo5)\n",
        "\n",
        ">>[Query](#scrollTo=VfbYyFHSRY3E)\n",
        "\n",
        ">>[Problem set 0](#scrollTo=Xp32ApAOXKu3)\n",
        "\n",
        ">>>[Cyberspace](#scrollTo=Xp32ApAOXKu3)\n",
        "\n",
        ">>>[Normals](#scrollTo=Xp32ApAOXKu3)\n",
        "\n",
        ">>>[Players](#scrollTo=Xp32ApAOXKu3)\n",
        "\n",
        ">>[Relating](#scrollTo=kn53eyiSJtIo)\n",
        "\n",
        ">>>[Entity Relationship Diagrams](#scrollTo=PeYhsqHJKTHg)\n",
        "\n",
        ">>>[Keys](#scrollTo=mf42AqgeNKbB)\n",
        "\n",
        ">>>>[Primary Keys](#scrollTo=mf42AqgeNKbB)\n",
        "\n",
        ">>>>[Foreign Keys](#scrollTo=mf42AqgeNKbB)\n",
        "\n",
        ">>>[Subqueries](#scrollTo=o-nS0PpuIDIB)\n",
        "\n",
        ">>>[IN](#scrollTo=VMJRqx0gIXu3)\n",
        "\n",
        ">>>[JOIN](#scrollTo=OHFKCV5rKdKa)\n",
        "\n",
        ">>>[Sets](#scrollTo=LLYTDDKCUo3j)\n",
        "\n",
        ">>>[Groups](#scrollTo=7O-AMBjEWo5A)\n",
        "\n",
        ">>>[Questions](#scrollTo=7O-AMBjEWo5A)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "fFLCTD40q2jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is ELT ?"
      ],
      "metadata": {
        "id": "Cf76ae0IeEcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Engineering for short\n",
        "It's about the practice of designing and building systems with purposes:\n",
        "1. collecting,\n",
        "2. storing,\n",
        "3. analyzing data at scale.\n",
        "4. To ensure the highly usable state before being pushed to data scientists, data analysts.\n",
        "\n",
        "[Coursera-What is Data Engineering](https://www.coursera.org/articles/what-does-a-data-engineer-do-and-how-do-i-become-one)\n"
      ],
      "metadata": {
        "id": "2xn5RpTpae9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Data Engineering\n",
        "\"Data engineering is a set of operations aimed at creating ***interfaces and mechanisms for the flow and access of information***. It takes dedicated specialists—data engineers— to maintain data so that it remains available and usable by others. In short, data engineers set up and operate the organization’s data infrastructure, preparing it for further analysis by data analysts and scientists\"\n",
        "\n",
        "-from “Data Engineering and Its Main Concepts” by AlexSoft\n",
        "\n",
        "**1. Data Engineering Defined**\n",
        "\n",
        "Data engineering is the development, implementation, and maintenance of systems\n",
        "and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering is the intersection of security, data management, DataOps, data architecture, orchestration, and software engineering. A ***data engineer manages the data engineering lifecycle***, beginning with getting data from source systems and ending with serving data for use cases, such as analysis or machine learning.\n",
        "\n",
        "**2. The Data Engineering Lifecycle**\n",
        "\n",
        "- The data engineering lifecycle focus on data itself and shift away the conversation away from technology.\n",
        "- There are many stages (above and below the iceberge):\n",
        "    - Generation, Storage, Serving: tasks of data engineering workflow itself.\n",
        "    - Security, DataOps, Software Engineering: tasks for interact with relevant fields\n",
        "\n",
        "![Data Engineering Life Cycle](https://github.com/newfrogg/data_engineering/blob/what_is_ELT/images/data_engineering_life_cycle.png?raw=1)\n",
        "\n",
        "**3. Evolution of the Data Engineer**\n",
        "\n",
        "*   The roots of data engineering can be traced back to the **data warehousing era (1980s-2000)**, pioneered by figures like Bill Inmon and Ralph Kimball. Early roles like BI engineers and ETL developers focused on building systems for scalable analytics using relational databases and MPP systems-. The rise of the web introduced new data scale challenges that traditional systems struggled with.\n",
        "*   **Contemporary data engineering emerged in the early 2000s** as companies faced exploding data growth. Innovations from Google (GFS, MapReduce), the open-source Hadoop ecosystem inspired by Google's work, and the advent of public clouds like AWS provided the **foundation for distributed computation and storage** on massive clusters-. This marked the beginning of the \"big data\" era.\n",
        "*   The **big data era (2000s-2010s)** saw the rise of the \"big data engineer\" proficient in software development and low-level infrastructure hacking to manage complex open-source tools like Hadoop and Spark-. While powerful, managing these massive clusters was operationally burdensome and costly, often diverting engineers from delivering business value. The term \"big data\" has since become a relic as the technology became more accessible.\n",
        "*   In the **2020s, big data engineers are now simply called data engineers or data lifecycle engineers**, reflecting a shift towards managing the entire data engineering lifecycle rather than low-level infrastructure details-. With greater abstraction and simplification of tools, the focus has moved to higher-value areas like security, data management, DataOps, data architecture, and orchestration. Data engineering has become a discipline of **connecting various modular technologies** to serve business goals.\n",
        "\n",
        "**4. Data Engineering and Data Science**\n",
        "\n",
        "- There are many opinions abouth relationship between data engineering and data science. However, for this case, we consider that **data engineering is a separate discipline from data science and analytics**, although they are complementary. Data engineering is described as sitting **upstream** from data science, meaning data engineers are responsible for providing the necessary data inputs for data scientists.\n",
        "\n",
        "- Considering **Data Science Hierarchy of Needs** published in 2017 by Monica Rogati, which places AI and machine learning at the top, with foundational tasks like data movement, storage, collection, cleansing, and infrastructure at the bottom. The sources state that data scientists often spend a significant majority of their time, estimated at **70% to 80%, on these lower-level tasks** such as gathering, cleaning, and processing data. This occurs because data scientists are typically not trained to engineer production-grade data systems.\n",
        "\n",
        "![Data Science Hierarchy](https://github.com/newfrogg/data_engineering/blob/what_is_ELT/images/data_science_hierarchy.png?raw=1)\n",
        "\n",
        "- The core idea is that **data engineers build the solid data foundation** represented by the bottom layers of this hierarchy. By doing so, data engineers enable data scientists to focus their time more effectively on higher-value activities like analysis, experimentation, and machine learning (the top layers of the pyramid). Ultimately, data engineering bridges the gap between acquiring raw data and extracting value from it, playing a vital role in the success of data science in production environments.\n",
        "\n",
        "[Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/)"
      ],
      "metadata": {
        "id": "Gvwe2gkoTv8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is ETL ?\n",
        "ETL is about a *data integration process* including:\n",
        "1. **Extract** data from legacy system\n",
        "2. **Transform** and/or clean data to enhance data quality, improve consitency\n",
        "3. **Load** data into target databases\n",
        "\n",
        "[IBM_ETL](https://www.ibm.com/think/topics/etl)\n",
        "\n"
      ],
      "metadata": {
        "id": "RBSCyNIkeHxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Amazon Web scraping\n",
        "Working though scraping data about books matching keyword \"Data Engineering\" from an ecommerce platform - Amazon to practice ETL process with html data.\n",
        "1. **Extract**: Search for book data from Amazon, staring with searching \"Data Engineering\" then scraping the data of each webpages like price, ratings, reviews, prices...\n",
        "2. **Transform**: Pull extracted data to an .csv file for storage and later usage\n",
        "3. <font color='red'>**Load**</font>: This example doesn't show the target database. So this phase is still left open for being defined later.\n",
        "\n",
        "Reference: https://github.com/kunal-geeks/amazon-web-scraping/tree/main"
      ],
      "metadata": {
        "id": "6W_OdARtpRMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "vnnrplBvphKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Function\n",
        "# Function to extract Product Title\n",
        "def get_title(soup):\n",
        "\n",
        "    try:\n",
        "        # Outer Tag Object\n",
        "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
        "\n",
        "        # Inner NavigatableString Object\n",
        "        title_value = title.text\n",
        "\n",
        "        # Title as a string value\n",
        "        title_string = title_value.strip()\n",
        "\n",
        "    except AttributeError:\n",
        "        title_string = \"\"\n",
        "\n",
        "    return title_string\n",
        "\n",
        "# Function to extract Product Price\n",
        "def get_price(soup):\n",
        "\n",
        "    try:\n",
        "        price = soup.find(\"div\",attrs={'class':'a-section aok-hidden twister-plus-buying-options-price-data'}).string.strip()\n",
        "        price = json.loads(price)\n",
        "        price = price[\"desktop_buybox_group_1\"][0][\"displayPrice\"]\n",
        "        # price = soup.find(\"span\", attrs={'class':'a-size-base a-color-price a-color-price'}).string.strip()\n",
        "    except:\n",
        "        price = \"\"\n",
        "\n",
        "    return price\n",
        "\n",
        "# Function to extract Product Rating\n",
        "def get_rating(soup):\n",
        "\n",
        "    try:\n",
        "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
        "\n",
        "    except AttributeError:\n",
        "        try:\n",
        "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
        "        except:\n",
        "            rating = \"\"\n",
        "\n",
        "    return rating\n",
        "\n",
        "# Function to extract Number of User Reviews\n",
        "def get_review_count(soup):\n",
        "    try:\n",
        "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
        "\n",
        "    except AttributeError:\n",
        "        review_count = \"\"\n",
        "\n",
        "    return review_count\n",
        "\n",
        "# Function to extract Availability Status\n",
        "def get_availability(soup):\n",
        "    try:\n",
        "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
        "        available = available.find(\"span\").string.strip()\n",
        "\n",
        "    except AttributeError:\n",
        "        available = \"Not Available\"\n",
        "\n",
        "    return available\n",
        "\n",
        "# Function to extract pages_count (On going fixing - the content is not stable => can't catch correctly each time)\n",
        "def get_pages(soup):\n",
        "    try:\n",
        "        # method 1\n",
        "        pages = soup.find(\"ul\", attrs={'class':'a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list'})\n",
        "        if pages:\n",
        "            pages = pages.find_all(\"li\")[4]\n",
        "\n",
        "        if pages:\n",
        "            pages = pages.find(\"span\")\n",
        "\n",
        "        if pages:\n",
        "            pages = pages.find_all(\"span\")[1].string.strip()\n",
        "        pages = soup.find(\"a\",attrs={'aria-label':re.compile('Print length.*')})\n",
        "        # method 2\n",
        "        # pages = soup.find_all(\"a\",attrs={'aria-description':'Popover with more information about the attribute'})\n",
        "        # if pages:\n",
        "        #     pages = pages[0]\n",
        "        # print(pages)\n",
        "        # pages = pages.get('aria-label').string.strip()\n",
        "\n",
        "    except AttributeError:\n",
        "        pages = \"Not Available\"\n",
        "    return pages\n",
        "\n",
        "# Function to extract main author\n",
        "def get_author(soup):\n",
        "    try:\n",
        "        author = soup.find(\"div\", attrs={'id':'bylineInfo', 'class':'a-section a-spacing-micro bylineHidden feature'})\n",
        "        author = author.find(\"span\")\n",
        "        author = author.find(\"a\").string.strip()\n",
        "    except AttributeError:\n",
        "        author = \"Not Available\"\n",
        "    return author\n"
      ],
      "metadata": {
        "id": "IwYKzkoqprPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # add your user agent\n",
        "    HEADERS = ({'User-Agent':'', 'Accept-Language': 'en-US, en;q=0.5'})\n",
        "\n",
        "    # The webpage URL\n",
        "    URL = \"https://www.amazon.com/s?k=data+engineering&i=stripbooks-intl-ship\"\n",
        "\n",
        "    # HTTP Request\n",
        "    webpage = requests.get(URL, headers=HEADERS)\n",
        "\n",
        "    # Soup Object containing all data\n",
        "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
        "\n",
        "    # Fetch links as List of Tag Objects\n",
        "    links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
        "\n",
        "    # Store the links\n",
        "    links_list = []\n",
        "\n",
        "    # Loop for extracting links from Tag Objects\n",
        "    for link in links:\n",
        "            links_list.append(link.get('href'))\n",
        "\n",
        "    d = {\"title\":[], \"price\":[], \"rating\":[], \"reviews\":[],\"availability\":[], \"authors\":[]}\n",
        "\n",
        "    # Loop for extracting product details from each link\n",
        "    for link in links_list:\n",
        "        new_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
        "\n",
        "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
        "\n",
        "        # Function calls to display all necessary product information\n",
        "        d['title'].append(get_title(new_soup))\n",
        "        d['price'].append(get_price(new_soup))\n",
        "        d['rating'].append(get_rating(new_soup))\n",
        "        d['reviews'].append(get_review_count(new_soup))\n",
        "        d['availability'].append(get_availability(new_soup))\n",
        "        d['authors'].append(get_author(new_soup))\n",
        "        # break\n",
        "\n",
        "\n",
        "    amazon_df = pd.DataFrame.from_dict(d)\n",
        "    amazon_df['title'].replace('', np.nan, inplace=True)\n",
        "\n",
        "\n",
        "    amazon_df = pd.DataFrame.from_dict(d)\n",
        "    amazon_df.replace({'title':''},np.nan, inplace=True)\n",
        "    amazon_df = amazon_df.dropna(subset=['title'])\n",
        "    amazon_df.to_csv(\"amazon_data.csv\", header=True, index=False)"
      ],
      "metadata": {
        "id": "aXZ0kZy5p2AM",
        "outputId": "6909ded6-3507-492c-a90e-10ac12fa402a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2588403658.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  amazon_df['title'].replace('', np.nan, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_df"
      ],
      "metadata": {
        "id": "8uAGHjRop7uz",
        "outputId": "ce2c3883-8151-4198-b391-8870f5517fc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                title   price  \\\n",
              "0   Fundamentals of Data Engineering: Plan and Bui...           \n",
              "1   Data Engineering Design Patterns: Recipes for ...  $51.47   \n",
              "2   AI Engineering: Building Applications with Fou...  $57.74   \n",
              "3   Data Pipelines Pocket Reference: Moving and Pr...  $16.93   \n",
              "4   Designing Data-Intensive Applications: The Big...  $43.23   \n",
              "5   Ace the Data Engineering Interview: Questions ...  $29.99   \n",
              "6   Data Engineering Best Practices: Architect rob...  $35.99   \n",
              "7   Financial Data Engineering: Design and Build D...  $49.20   \n",
              "8   Databricks Certified Data Engineer Associate S...  $64.59   \n",
              "9   Data Engineering with Python: Work with massiv...  $41.99   \n",
              "10  Data Engineering Fundamentals: Building scalab...           \n",
              "11  Data Engineering with dbt: A practical guide t...  $37.20   \n",
              "12                         Snowflake Data Engineering  $49.84   \n",
              "13  The Data Engineering Handbook: We are Data Eng...   $9.99   \n",
              "14  Data Engineering with Google Cloud Platform: A...  $24.33   \n",
              "15  Data Engineering with AWS: Acquire the skills ...           \n",
              "\n",
              "                rating        reviews                        availability  \\\n",
              "0   4.7 out of 5 stars    703 ratings                       Not Available   \n",
              "1   4.4 out of 5 stars      2 ratings  Only 1 left in stock - order soon.   \n",
              "2   4.4 out of 5 stars    220 ratings                            In Stock   \n",
              "3   4.4 out of 5 stars    401 ratings                            In Stock   \n",
              "4   4.4 out of 5 stars  5,200 ratings                            In Stock   \n",
              "5   4.4 out of 5 stars      3 ratings                            In Stock   \n",
              "6   4.4 out of 5 stars      4 ratings                            In Stock   \n",
              "7   4.4 out of 5 stars      8 ratings                            In Stock   \n",
              "8   4.4 out of 5 stars     11 ratings                            In Stock   \n",
              "9   4.4 out of 5 stars    146 ratings                            In Stock   \n",
              "10  4.4 out of 5 stars      2 ratings                       Not Available   \n",
              "11  4.8 out of 5 stars     25 ratings                            In Stock   \n",
              "12  4.7 out of 5 stars      7 ratings                            In Stock   \n",
              "13  4.4 out of 5 stars      3 ratings                            In Stock   \n",
              "14  4.4 out of 5 stars     16 ratings                            In Stock   \n",
              "15  4.4 out of 5 stars     50 ratings                       Not Available   \n",
              "\n",
              "                authors  \n",
              "0              Joe Reis  \n",
              "1     Bartosz Konieczny  \n",
              "2            Chip Huyen  \n",
              "3        James Densmore  \n",
              "4      Martin Kleppmann  \n",
              "5            Sean Coyne  \n",
              "6   Richard J. Schiller  \n",
              "7        Tamer Khraisha  \n",
              "8       Derar Alhussein  \n",
              "9         Paul Crickard  \n",
              "10         Zhaolong Liu  \n",
              "11        Roberto Zagni  \n",
              "12           Maja Ferle  \n",
              "13            Joe Trite  \n",
              "14           Adi Wijaya  \n",
              "15         Gareth Eagar  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e91b717f-b87b-497e-ae68-ff3eeaaada66\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>price</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviews</th>\n",
              "      <th>availability</th>\n",
              "      <th>authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fundamentals of Data Engineering: Plan and Bui...</td>\n",
              "      <td></td>\n",
              "      <td>4.7 out of 5 stars</td>\n",
              "      <td>703 ratings</td>\n",
              "      <td>Not Available</td>\n",
              "      <td>Joe Reis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Engineering Design Patterns: Recipes for ...</td>\n",
              "      <td>$51.47</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>2 ratings</td>\n",
              "      <td>Only 1 left in stock - order soon.</td>\n",
              "      <td>Bartosz Konieczny</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI Engineering: Building Applications with Fou...</td>\n",
              "      <td>$57.74</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>220 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Chip Huyen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data Pipelines Pocket Reference: Moving and Pr...</td>\n",
              "      <td>$16.93</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>401 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>James Densmore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Designing Data-Intensive Applications: The Big...</td>\n",
              "      <td>$43.23</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>5,200 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Martin Kleppmann</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ace the Data Engineering Interview: Questions ...</td>\n",
              "      <td>$29.99</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>3 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Sean Coyne</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Data Engineering Best Practices: Architect rob...</td>\n",
              "      <td>$35.99</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>4 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Richard J. Schiller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Financial Data Engineering: Design and Build D...</td>\n",
              "      <td>$49.20</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>8 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Tamer Khraisha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Databricks Certified Data Engineer Associate S...</td>\n",
              "      <td>$64.59</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>11 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Derar Alhussein</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Data Engineering with Python: Work with massiv...</td>\n",
              "      <td>$41.99</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>146 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Paul Crickard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Data Engineering Fundamentals: Building scalab...</td>\n",
              "      <td></td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>2 ratings</td>\n",
              "      <td>Not Available</td>\n",
              "      <td>Zhaolong Liu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Data Engineering with dbt: A practical guide t...</td>\n",
              "      <td>$37.20</td>\n",
              "      <td>4.8 out of 5 stars</td>\n",
              "      <td>25 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Roberto Zagni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Snowflake Data Engineering</td>\n",
              "      <td>$49.84</td>\n",
              "      <td>4.7 out of 5 stars</td>\n",
              "      <td>7 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Maja Ferle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The Data Engineering Handbook: We are Data Eng...</td>\n",
              "      <td>$9.99</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>3 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Joe Trite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Data Engineering with Google Cloud Platform: A...</td>\n",
              "      <td>$24.33</td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>16 ratings</td>\n",
              "      <td>In Stock</td>\n",
              "      <td>Adi Wijaya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Data Engineering with AWS: Acquire the skills ...</td>\n",
              "      <td></td>\n",
              "      <td>4.4 out of 5 stars</td>\n",
              "      <td>50 ratings</td>\n",
              "      <td>Not Available</td>\n",
              "      <td>Gareth Eagar</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e91b717f-b87b-497e-ae68-ff3eeaaada66')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e91b717f-b87b-497e-ae68-ff3eeaaada66 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e91b717f-b87b-497e-ae68-ff3eeaaada66');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-afd1a7fc-ab89-4708-9589-0b4827badcb4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afd1a7fc-ab89-4708-9589-0b4827badcb4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-afd1a7fc-ab89-4708-9589-0b4827badcb4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b95e3feb-af53-4edd-82f9-a84091684d91\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('amazon_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b95e3feb-af53-4edd-82f9-a84091684d91 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('amazon_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "amazon_df",
              "summary": "{\n  \"name\": \"amazon_df\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"Fundamentals of Data Engineering: Plan and Build Robust Data Systems\",\n          \"Data Engineering Design Patterns: Recipes for Solving the Most Common Data Engineering Problems\",\n          \"Ace the Data Engineering Interview: Questions and Answers for Python, SQL, Data Modeling and More\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"$41.99\",\n          \"$49.84\",\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"4.7 out of 5 stars\",\n          \"4.4 out of 5 stars\",\n          \"4.8 out of 5 stars\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"146 ratings\",\n          \"7 ratings\",\n          \"703 ratings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"availability\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Not Available\",\n          \"Only 1 left in stock - order soon.\",\n          \"In Stock\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"Joe Reis\",\n          \"Bartosz Konieczny\",\n          \"Sean Coyne\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review Python\n",
        "Below is a 1-2 week project-based learning plan to review Python basics up to lists and tuples, tailored for data engineering and data analysis. The project involves building **a simple data pipeline** to process and analyze a dataset of customer transactions, reinforcing Python fundamentals while introducing data-related concepts."
      ],
      "metadata": {
        "id": "cR6dB4EQ_NYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_csv(file_path):\n",
        "    data = [\n",
        "        \"transaction_id,customer_name,amount,date,category\",\n",
        "        \"1,John Doe,50.25,2025-06-01,Food\",\n",
        "        \"2,Jane Smith,120.50,2025-06-02,Electronics\",\n",
        "        \"3,Alice Johnson,30.00,2025-06-03,Clothing\",\n",
        "        \"4,John Doe,75.80,2025-06-03,Food\",\n",
        "        \"5,Bob Wilson,200.00,2025-06-04,Electronics\",\n",
        "        \"6,Jane Smith,15.99,2025-06-05,Books\",\n",
        "        \"7,Alice Johnson,45.60,2025-06-06,Clothing\",\n",
        "        \"8,John Doe,-10.00,2025-06-07,Food\",\n",
        "        \"9,Emma Davis,80.00,2025-06-08,Electronics\",\n",
        "        \"10,Bob Wilson,25.50,2025-06-09,Books\",\n",
        "        \"11,Jane Smith,60.75,2025-06-10,Food\",\n",
        "        \"12,Invalid User,abc,2025-06-11,Clothing\",\n",
        "        \"13,Emma Davis,90.20,2025-06-12,Electronics\",\n",
        "        \"14,John Doe,35.40,2025-06-13,Books\",\n",
        "        \"15,Alice Johnson,70.00,2025-06-14,Clothing\"\n",
        "    ]\n",
        "    with open(file_path, 'w') as file:\n",
        "        for line in data:\n",
        "            file.write(line + '\\n')\n",
        "    print(f\"Sample CSV created at {file_path} as below\")\n",
        "\n",
        "create_sample_csv('transactions.csv')\n",
        "!cat transactions.csv | column -t -s \",\""
      ],
      "metadata": {
        "id": "FyN8fAl_AIMq",
        "outputId": "3eba5487-414d-42de-b362-a65a7cda111a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample CSV created at transactions.csv as below\n",
            "transaction_id  customer_name  amount  date        category\n",
            "1               John Doe       50.25   2025-06-01  Food\n",
            "2               Jane Smith     120.50  2025-06-02  Electronics\n",
            "3               Alice Johnson  30.00   2025-06-03  Clothing\n",
            "4               John Doe       75.80   2025-06-03  Food\n",
            "5               Bob Wilson     200.00  2025-06-04  Electronics\n",
            "6               Jane Smith     15.99   2025-06-05  Books\n",
            "7               Alice Johnson  45.60   2025-06-06  Clothing\n",
            "8               John Doe       -10.00  2025-06-07  Food\n",
            "9               Emma Davis     80.00   2025-06-08  Electronics\n",
            "10              Bob Wilson     25.50   2025-06-09  Books\n",
            "11              Jane Smith     60.75   2025-06-10  Food\n",
            "12              Invalid User   abc     2025-06-11  Clothing\n",
            "13              Emma Davis     90.20   2025-06-12  Electronics\n",
            "14              John Doe       35.40   2025-06-13  Books\n",
            "15              Alice Johnson  70.00   2025-06-14  Clothing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf transactions.*\n",
        "!echo \"transaction_id,customer_name,amount,date,category\" >> transactions.csv\n",
        "!echo \"1,John Doe,50.25,2025-06-01,Food\" >> transactions.csv\n",
        "!echo \"2,Jane Smith,120.50,2025-06-02,Electronics\" >> transactions.csv\n",
        "!echo \"3,Alice Johnson,30.00,2025-06-03,Clothing\" >> transactions.csv\n",
        "!echo \"4,John Doe,75.80,2025-06-03,Food\" >> transactions.csv\n",
        "!echo \"5,Bob Wilson,200.00,2025-06-04,Electronics\" >> transactions.csv\n",
        "!echo \"6,Jane Smith,15.99,2025-06-05,Books\" >> transactions.csv\n",
        "!echo \"7,Alice Johnson,45.60,2025-06-06,Clothing\" >> transactions.csv\n",
        "!echo \"8,John Doe,-10.00,2025-06-07,Food\" >> transactions.csv\n",
        "!echo \"9,Emma Davis,80.00,2025-06-08,Electronics\" >> transactions.csv\n",
        "!echo \"10,Bob Wilson,25.50,2025-06-09,Books\" >> transactions.csv\n",
        "!echo \"11,Jane Smith,60.75,2025-06-10,Food\" >> transactions.csv\n",
        "!echo \"12,Invalid User,abc,2025-06-11,Clothing\" >> transactions.csv\n",
        "!echo \"13,Emma Davis,90.20,2025-06-12,Electronics\" >> transactions.csv\n",
        "!echo \"14,John Doe,35.40,2025-06-13,Books\" >> transactions.csv\n",
        "!echo \"15,Alice Johnson,70.00,2025-06-14,Clothing\" >> transactions.csv\n",
        "!echo \"====================================================================\"\n",
        "!echo \"======================== Transaction Table =========================\"\n",
        "!echo \"====================================================================\"\n",
        "!cat transactions.csv | column -t -s \",\" >> transactions.table\n",
        "!cat transactions.table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXg82lV7NSW7",
        "outputId": "65bfafc8-89b8-4ceb-8c47-7f23a23f67b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================\n",
            "======================== Transaction Table =========================\n",
            "====================================================================\n",
            "transaction_id  customer_name  amount  date        category\n",
            "1               John Doe       50.25   2025-06-01  Food\n",
            "2               Jane Smith     120.50  2025-06-02  Electronics\n",
            "3               Alice Johnson  30.00   2025-06-03  Clothing\n",
            "4               John Doe       75.80   2025-06-03  Food\n",
            "5               Bob Wilson     200.00  2025-06-04  Electronics\n",
            "6               Jane Smith     15.99   2025-06-05  Books\n",
            "7               Alice Johnson  45.60   2025-06-06  Clothing\n",
            "8               John Doe       -10.00  2025-06-07  Food\n",
            "9               Emma Davis     80.00   2025-06-08  Electronics\n",
            "10              Bob Wilson     25.50   2025-06-09  Books\n",
            "11              Jane Smith     60.75   2025-06-10  Food\n",
            "12              Invalid User   abc     2025-06-11  Clothing\n",
            "13              Emma Davis     90.20   2025-06-12  Electronics\n",
            "14              John Doe       35.40   2025-06-13  Books\n",
            "15              Alice Johnson  70.00   2025-06-14  Clothing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview\n",
        "- **Objective**: Create a Python script that reads a CSV file of customer transactions, cleans the data, and generates basic analytical insights (e.g., total sales, top customers).\n",
        "- **Dataset**: Use a sample CSV file with columns: `transaction_id`, `customer_name`, `amount`, `date`, `category`.\n",
        "- **Tools**: Python, `csv` module (standard library).\n",
        "- **Duration**: 1-2 weeks, ~2-3 hours/day."
      ],
      "metadata": {
        "id": "qLmqhyMrNBu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Week 1: Python Basics and Data Ingestion\n"
      ],
      "metadata": {
        "id": "LMYJy-QAWQa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Day 1: Setup and Python Basics\n",
        "- **Topics**: Variables, data types (int, float, str, bool), basic operations, input/output.\n",
        "- **Task**: Set up Python environment, create a sample CSV file, and write a script to read it.\n",
        "- **Activity**:\n",
        "  - Create a CSV file `transactions.csv` with 10-15 rows (e.g., `1,John Doe,50.25,2025-06-01,Food`).\n",
        "  - Write a script to open and print the file content.\n",
        "- **Code**:\n",
        "  ```python\n",
        "  # Read and print CSV file\n",
        "  with open('transactions.csv', 'r') as file:\n",
        "      for line in file:\n",
        "          print(line.strip())\n",
        "  ```\n",
        "\n",
        "**Close Files**\n",
        "\n",
        "It is a good practice to always close the file when you are done with it.\n",
        "\n",
        "If you are not using the with statement, you must write a close statement in order to close the file:\n",
        "  ```python\n",
        "  f = open(\"demofile.txt\")\n",
        "  print(f.readline())\n",
        "  f.close()\n",
        "  ```\n",
        "\n",
        "**Strip function**\n",
        "\n",
        "The strip() method removes any leading, and trailing whitespaces.\n",
        "\n",
        "Leading means at the beginning of the string, trailing means at the end.\n",
        "\n",
        "You can specify which character(s) to remove, if not, any whitespaces will be removed.\n",
        "\n",
        "**Syntax**: *string.strip(characters)*\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "txt = \",,,,,rrttgg.....banana....rrr\"\n",
        "x = txt.strip(\",.grt\")\n",
        "print(x)\n",
        "```\n"
      ],
      "metadata": {
        "id": "ppD7QXboNHV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('transactions.csv', 'r') as file:\n",
        "    print(\"case 1: w/o strip function\")\n",
        "    for line in file:\n",
        "        print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apXlDhhdQist",
        "outputId": "e55d8d17-d80d-49eb-8607-afd7e9bceeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 1: w/o strip function\n",
            "transaction_id,customer_name,amount,date,category\n",
            "\n",
            "1,John Doe,50.25,2025-06-01,Food\n",
            "\n",
            "2,Jane Smith,120.50,2025-06-02,Electronics\n",
            "\n",
            "3,Alice Johnson,30.00,2025-06-03,Clothing\n",
            "\n",
            "4,John Doe,75.80,2025-06-03,Food\n",
            "\n",
            "5,Bob Wilson,200.00,2025-06-04,Electronics\n",
            "\n",
            "6,Jane Smith,15.99,2025-06-05,Books\n",
            "\n",
            "7,Alice Johnson,45.60,2025-06-06,Clothing\n",
            "\n",
            "8,John Doe,-10.00,2025-06-07,Food\n",
            "\n",
            "9,Emma Davis,80.00,2025-06-08,Electronics\n",
            "\n",
            "10,Bob Wilson,25.50,2025-06-09,Books\n",
            "\n",
            "11,Jane Smith,60.75,2025-06-10,Food\n",
            "\n",
            "12,Invalid User,abc,2025-06-11,Clothing\n",
            "\n",
            "13,Emma Davis,90.20,2025-06-12,Electronics\n",
            "\n",
            "14,John Doe,35.40,2025-06-13,Books\n",
            "\n",
            "15,Alice Johnson,70.00,2025-06-14,Clothing\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('transactions.csv', 'r') as file:\n",
        "    print(\"case 2: with strip function\")\n",
        "    for line in file:\n",
        "        print(line.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b2vB7gTOXWa",
        "outputId": "80ec11db-1cf5-4f03-cd13-4974fccf29e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "case 2: with strip function\n",
            "transaction_id,customer_name,amount,date,category\n",
            "1,John Doe,50.25,2025-06-01,Food\n",
            "2,Jane Smith,120.50,2025-06-02,Electronics\n",
            "3,Alice Johnson,30.00,2025-06-03,Clothing\n",
            "4,John Doe,75.80,2025-06-03,Food\n",
            "5,Bob Wilson,200.00,2025-06-04,Electronics\n",
            "6,Jane Smith,15.99,2025-06-05,Books\n",
            "7,Alice Johnson,45.60,2025-06-06,Clothing\n",
            "8,John Doe,-10.00,2025-06-07,Food\n",
            "9,Emma Davis,80.00,2025-06-08,Electronics\n",
            "10,Bob Wilson,25.50,2025-06-09,Books\n",
            "11,Jane Smith,60.75,2025-06-10,Food\n",
            "12,Invalid User,abc,2025-06-11,Clothing\n",
            "13,Emma Davis,90.20,2025-06-12,Electronics\n",
            "14,John Doe,35.40,2025-06-13,Books\n",
            "15,Alice Johnson,70.00,2025-06-14,Clothing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day 2: Lists and Basic Data Structures\n",
        "- **Topics**: Lists, indexing, slicing, list methods (append, remove, sort).\n",
        "- **Task**: Parse CSV rows into a list of lists, extract specific columns.\n",
        "- **Activity**:\n",
        "  - Modify the script to store each row as a list in a master list.\n",
        "  - Print all transaction amounts (column 2).\n",
        "- **Code**:\n",
        "  ```python\n",
        "  transactions = []\n",
        "  with open('transactions.csv', 'r') as file:\n",
        "      for line in file:\n",
        "          row = line.strip().split(',')\n",
        "          transactions.append(row)\n",
        "  amounts = [float(row[2]) for row in transactions[1:]]  # Skip header\n",
        "  print(amounts)\n",
        "  ```"
      ],
      "metadata": {
        "id": "VTxwG1VqTMsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = []\n",
        "with open('transactions.csv', 'r') as file:\n",
        "    for line in file:\n",
        "        row = line.strip().split(',')\n",
        "        transactions.append(row)\n",
        "amounts = [float(row[2]) for row in transactions[1:]]  # Skip header\n",
        "print(amounts)"
      ],
      "metadata": {
        "id": "Hh33Yo3ETTkC",
        "outputId": "1320e0a9-4029-4a4d-d633-440bed1cbc7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'abc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-778779406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtransactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mamounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Skip header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-778779406.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtransactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mamounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Skip header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'abc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This above errors caused by invalid value in amount column**.\n",
        "\n",
        "We try to apply try and catch to cover this unexpected behavior as below code.\n",
        "\n",
        "![Invalid amount data](https://github.com/newfrogg/data_engineering/blob/review_python/images/invalid_amount.png?raw=1)"
      ],
      "metadata": {
        "id": "K5BQThkWYWMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = []\n",
        "with open('transactions.csv', 'r') as file:\n",
        "    for line in file:\n",
        "        row = line.strip().split(',')\n",
        "        transactions.append(row)\n",
        "\n",
        "amount_original = []\n",
        "amount_modified = []\n",
        "## Try retrieve the amount value and convert to true format number.\n",
        "for index, row in enumerate(transactions[1:]):\n",
        "    amount_original.append(row[2])\n",
        "    try:\n",
        "        print(f\"Infomation: Convert amount {row[2]} from String to Float successfully\")\n",
        "        amount_modified.append(float(row[2]))\n",
        "    except ValueError:\n",
        "        print(f\"Warning: Invalid amount: {row[2]} => Change to default value (0)\")\n",
        "        amount_modified.append(float(0.0))\n",
        "\n",
        "!echo \"<<< Original column 'amount'\"\n",
        "print(amount_original)\n",
        "!echo \">>> Modified column 'amount'\"\n",
        "print(amount_modified)"
      ],
      "metadata": {
        "id": "eaecIpFNUeiS",
        "outputId": "1171cb47-6cfe-4cc6-a850-c7d12f06841f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infomation: Convert amount 50.25 from String to Float successfully\n",
            "Infomation: Convert amount 120.50 from String to Float successfully\n",
            "Infomation: Convert amount 30.00 from String to Float successfully\n",
            "Infomation: Convert amount 75.80 from String to Float successfully\n",
            "Infomation: Convert amount 200.00 from String to Float successfully\n",
            "Infomation: Convert amount 15.99 from String to Float successfully\n",
            "Infomation: Convert amount 45.60 from String to Float successfully\n",
            "Infomation: Convert amount -10.00 from String to Float successfully\n",
            "Infomation: Convert amount 80.00 from String to Float successfully\n",
            "Infomation: Convert amount 25.50 from String to Float successfully\n",
            "Infomation: Convert amount 60.75 from String to Float successfully\n",
            "Infomation: Convert amount abc from String to Float successfully\n",
            "Warning: Invalid amount: abc => Change to default value (0)\n",
            "Infomation: Convert amount 90.20 from String to Float successfully\n",
            "Infomation: Convert amount 35.40 from String to Float successfully\n",
            "Infomation: Convert amount 70.00 from String to Float successfully\n",
            "<<< Original column 'amount'\n",
            "['50.25', '120.50', '30.00', '75.80', '200.00', '15.99', '45.60', '-10.00', '80.00', '25.50', '60.75', 'abc', '90.20', '35.40', '70.00']\n",
            ">>> Modified column 'amount'\n",
            "[50.25, 120.5, 30.0, 75.8, 200.0, 15.99, 45.6, -10.0, 80.0, 25.5, 60.75, 0.0, 90.2, 35.4, 70.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day 3: Conditionals and Data Cleaning\n",
        "- **Topics**: If-else statements, comparison operators, handling missing data.\n",
        "- **Task**: Identify and handle invalid entries (e.g., negative amounts, empty fields).\n",
        "- **Activity**:\n",
        "  - Add checks to skip rows with invalid amounts (e.g., negative or non-numeric).\n",
        "  - Log invalid rows to a list.\n",
        "  \n",
        "We try to separate transaction have wrong value. By filtering data from ***amount column***, we eliminate 2 invalid out of 15 column at all."
      ],
      "metadata": {
        "id": "c20qkDHXOp2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_rows = []\n",
        "valid_transactions = []\n",
        "with open('transactions.csv', 'r') as file:\n",
        "    next(file)  # Skip header\n",
        "    for line in file:\n",
        "        row = line.strip().split(',')\n",
        "        try:\n",
        "            amount = float(row[2])\n",
        "            if amount < 0:\n",
        "                invalid_rows.append(row)\n",
        "            else:\n",
        "                valid_transactions.append(row)\n",
        "        except ValueError:\n",
        "            invalid_rows.append(row)\n",
        "print(f\"Valid transactions: {len(valid_transactions)} of {len(valid_transactions) + len(invalid_rows)}\")\n",
        "print(f\"Invalid transactions: {len(invalid_rows)}\")\n",
        "\n",
        "print(invalid_rows)\n"
      ],
      "metadata": {
        "id": "3LR6T4FNOruN",
        "outputId": "d62d8cf7-e004-48f9-cdaf-8be60fb5dacc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid transactions: 13 of 15\n",
            "Invalid transactions: 2\n",
            "[['8', 'John Doe', '-10.00', '2025-06-07', 'Food'], ['12', 'Invalid User', 'abc', '2025-06-11', 'Clothing']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(invalid_rows)"
      ],
      "metadata": {
        "id": "VL5YjT3CTvgP",
        "outputId": "0bdf14a1-eb5e-4919-c6fa-587a4c314270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['8', 'John Doe', '-10.00', '2025-06-07', 'Food'], ['12', 'Invalid User', 'abc', '2025-06-11', 'Clothing']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[print(row) for row in invalid_rows]"
      ],
      "metadata": {
        "id": "QTYzu5MLSHKT",
        "outputId": "da5ce1fa-8390-4bc4-b7db-14485781442f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['8', 'John Doe', '-10.00', '2025-06-07', 'Food']\n",
            "['12', 'Invalid User', 'abc', '2025-06-11', 'Clothing']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **issue** you're describing arises because the list **comprehension print(row) for row in invalid_rows** prints each row but also returns a list of the results of the print() function, which is None for each call. **This is why you see [None, None] in the output after the rows are printed.**\n",
        "\n",
        "#### **Why does it print None, None?**\n",
        "The print(row) function outputs the row to the console and returns None.\n",
        "The list comprehension creates a list of these return values (None for each row).\n",
        "```python\n",
        "[print(row) for row in invalid_rows]\n",
        "```\n",
        "For your invalid_rows with two rows, the comprehension results in [None, None], which is then displayed as part of the output.\n",
        "\n",
        "Solution:\n",
        "1. Use an explicit for loop\n",
        "```python\n",
        "for row in invalid_rows:\n",
        "    print(row)\n",
        "```\n",
        "2. Use a side-effect-free comprehension\n",
        "```python\n",
        "_ = [print(row) for row in invalid_rows]\n",
        "```"
      ],
      "metadata": {
        "id": "Uv5MH40NUdsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_= [print(row) for row in invalid_rows]"
      ],
      "metadata": {
        "id": "wjieqWfkSEvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc92dad-e830-4715-98f0-89f83ad4df2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['8', 'John Doe', '-10.00', '2025-06-07', 'Food']\n",
            "['12', 'Invalid User', 'abc', '2025-06-11', 'Clothing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A side-effect-free comprehension focuses solely on transforming or filtering data to create a new collection (list, set, or dictionary) **without performing external operations like printing**. It adheres to the principle of functional programming, where operations are predictable and don’t affect external state.\n",
        "\n",
        "For example, a side-effect-free version of your comprehension would avoid print() and simply collect or transform the rows:\n",
        "\n",
        "#### **What is a Side Effect?**\n",
        "A side effect occurs when a function or operation does something beyond returning a value. In contrast, a pure function or operation has no side effects—it only computes and returns a value based on its inputs."
      ],
      "metadata": {
        "id": "JPRSPIw5V3Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Day 4: Loops and Data Aggregation\n",
        "- **Topics**: For loops, while loops, iterating over lists.\n",
        "- **Task**: Calculate total sales and count transactions per category.\n",
        "- **Activity**:\n",
        "  - Use a loop to sum transaction amounts.\n",
        "  - Create a list to store category counts.\n",
        "\n",
        "Using with allowing automatically close file after completing execute code inside this scope.\n",
        "```python\n",
        "with open('$file_name','mode) as file:\n",
        "    next(file) # Skip header\n",
        "```\n"
      ],
      "metadata": {
        "id": "UH9ZvdNgWKvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_sales = 0\n",
        "category_counts = []\n",
        "categories = []\n",
        "with open('transactions.csv', 'r') as file:\n",
        "    next(file)  # Skip header\n",
        "    for line in file:\n",
        "        row = line.strip().split(',')\n",
        "        try:\n",
        "            amount = float(row[2])\n",
        "            if amount >= 0:\n",
        "                total_sales += amount\n",
        "                category = row[4]\n",
        "                if category not in categories:\n",
        "                    categories.append(category)\n",
        "                    category_counts.append(1)\n",
        "                else:\n",
        "                    index = categories.index(category)\n",
        "                    category_counts[index] += 1\n",
        "        except ValueError:\n",
        "            continue\n",
        "print(f\"Total sales: ${total_sales:.2f}\")\n",
        "print(\"Category counts:\", list(zip(categories, category_counts)))"
      ],
      "metadata": {
        "id": "_eeDc6jmWdPr",
        "outputId": "afd6e676-bbdc-43f3-e004-83078a154106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sales: $899.99\n",
            "Category counts: [('Food', 3), ('Electronics', 4), ('Clothing', 3), ('Books', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **zip function help zip the lists** to each other that catch by minlenght of list. Then set list to make it an object readable. Example:\n",
        "```python\n",
        "a = [1,2,3]\n",
        "b = [2,45]\n",
        "c = [312, 121, 123,1 ,12]\n",
        "list(zip(a,b,c))\n",
        "# result>> [(1, 2, 312), (2, 45, 121)]\n",
        "```\n",
        "- A new approach that help clean and minimize that simple code. Obviously, when we control the Value Error of data\n",
        "```python\n",
        "total_sales = 0\n",
        "total_sales = sum(float(row[2]) for row in valid_transactions)\n",
        "print(f\"Total sales: ${total_sales:.2f}\")\n",
        "# result>> Total sales: $899.99\n",
        "```"
      ],
      "metadata": {
        "id": "89bsckniYbvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,2,3]\n",
        "b = [2,45]\n",
        "c = [312, 121, 123,1 ,12]\n",
        "list(zip(a,b,c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtM7vxszaYhP",
        "outputId": "bb60aa60-d46b-462c-843e-98de56fdb303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2, 312), (2, 45, 121)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **An alternative using pandas library**\n",
        "For this example, we can see the powerful of library, that cover corner cases, catch exceptions."
      ],
      "metadata": {
        "id": "ehSNeEIWbj6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read CSV into a DataFrame\n",
        "df = pd.read_csv('transactions.csv')\n",
        "\n",
        "# Convert 'Amount' to numeric, coercing errors to NaN\n",
        "df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
        "\n",
        "# Filter valid rows (non-negative amounts, non-NaN)\n",
        "valid_df = df[df['amount'] >= 0]\n",
        "\n",
        "# Calculate total sales\n",
        "total_sales = valid_df['amount'].sum()\n",
        "\n",
        "# Get category frequencies\n",
        "category_freq = valid_df['category'].value_counts().to_dict()\n",
        "\n",
        "print(f\"Total sales: ${total_sales:.2f}\")\n",
        "print(\"Category counts:\", list(category_freq.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_7h3dWqbMFu",
        "outputId": "8a4b1184-3b89-4b43-e131-5a27a8679bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sales: $899.99\n",
            "Category counts: [('Electronics', 4), ('Food', 3), ('Clothing', 3), ('Books', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day 5: Functions and Modularity\n",
        "- **Topics**: Defining functions, parameters, return values.\n",
        "- **Task**: Refactor code into functions for reusability.\n",
        "- **Activity**:\n",
        "  - Write functions for reading CSV, cleaning data, and calculating totals.\n",
        "  - Call functions in a main script.\n"
      ],
      "metadata": {
        "id": "gz3Y_nQkdLfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv(file_path):\n",
        "      transactions = []\n",
        "      with open(file_path, 'r') as file:\n",
        "          next(file)  # Skip header\n",
        "          for line in file:\n",
        "              transactions.append(line.strip().split(','))\n",
        "      return transactions\n",
        "\n",
        "def clean_data(transactions):\n",
        "    valid = []\n",
        "    invalid = []\n",
        "    for row in transactions:\n",
        "        try:\n",
        "            amount = float(row[2])\n",
        "            if amount >= 0:\n",
        "                valid.append(row)\n",
        "            else:\n",
        "                invalid.append(row)\n",
        "        except ValueError:\n",
        "            invalid.append(row)\n",
        "    return valid, invalid\n",
        "\n",
        "def calculate_totals(valid_transactions):\n",
        "    total_sales = sum(float(row[2]) for row in valid_transactions)\n",
        "    return total_sales\n",
        "\n",
        "# Main script\n",
        "## Reading data from CSV file into 2-dimension list\n",
        "transactions = read_csv('transactions.csv')\n",
        "## Cleanning invalid rows (contain string in numeric cells)\n",
        "valid, invalid = clean_data(transactions)\n",
        "## Sum of total_sales (valid)\n",
        "total_sales = calculate_totals(valid)\n",
        "print(f\"Total sales: ${total_sales:.2f}\")\n",
        "print(f\"Invalid rows: {len(invalid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9InGAAjodK2p",
        "outputId": "66a4e2a0-ee5c-47b1-b865-454acf4911ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sales: $899.99\n",
            "Invalid rows: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Week 2: Tuples, Analysis, and Pipeline Completion"
      ],
      "metadata": {
        "id": "I1iH51nDejBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day 6: Tuples and Immutable Data\n",
        "- **Topics**: Tuples, tuple operations, use cases.\n",
        "- **Task**: Store cleaned transactions as tuples for immutability.\n",
        "- **Activity**:\n",
        "  - Convert valid transactions to a list of tuples.\n",
        "  - Verify tuple immutability by attempting to modify one.\n",
        "\n",
        "In Python, a tuple is an immutable, ordered collection of items that can store elements of any data type (e.g., numbers, strings, lists, or even other tuples). **Tuples are similar to lists but cannot be modified after creation**, making them useful for representing fixed collections of data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9v22PN1Uen2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data_to_tuples(transactions):\n",
        "    valid = []\n",
        "    invalid = []\n",
        "    for row in transactions:\n",
        "        try:\n",
        "            amount = float(row[2])\n",
        "            if amount >= 0:\n",
        "                valid.append((row[0], row[1], amount, row[3], row[4]))\n",
        "            else:\n",
        "                invalid.append(row)\n",
        "        except ValueError:\n",
        "            invalid.append(row)\n",
        "    return valid, invalid\n",
        "\n",
        "transactions = read_csv('transactions.csv')\n",
        "valid_tuples, invalid = clean_data_to_tuples(transactions)\n",
        "print(\"First transaction:\", valid_tuples[2])\n",
        "print(\"Valid transactions under tuple form\")\n",
        "_ = [print(valid) for valid in valid_tuples]\n"
      ],
      "metadata": {
        "id": "mxZ5WctbewGd",
        "outputId": "6dd1aa88-0b31-4f83-e28f-fb2dec149e1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First transaction: ('3', 'Alice Johnson', 30.0, '2025-06-03', 'Clothing')\n",
            "Valid transactions under tuple form\n",
            "('1', 'John Doe', 50.25, '2025-06-01', 'Food')\n",
            "('2', 'Jane Smith', 120.5, '2025-06-02', 'Electronics')\n",
            "('3', 'Alice Johnson', 30.0, '2025-06-03', 'Clothing')\n",
            "('4', 'John Doe', 75.8, '2025-06-03', 'Food')\n",
            "('5', 'Bob Wilson', 200.0, '2025-06-04', 'Electronics')\n",
            "('6', 'Jane Smith', 15.99, '2025-06-05', 'Books')\n",
            "('7', 'Alice Johnson', 45.6, '2025-06-06', 'Clothing')\n",
            "('9', 'Emma Davis', 80.0, '2025-06-08', 'Electronics')\n",
            "('10', 'Bob Wilson', 25.5, '2025-06-09', 'Books')\n",
            "('11', 'Jane Smith', 60.75, '2025-06-10', 'Food')\n",
            "('13', 'Emma Davis', 90.2, '2025-06-12', 'Electronics')\n",
            "('14', 'John Doe', 35.4, '2025-06-13', 'Books')\n",
            "('15', 'Alice Johnson', 70.0, '2025-06-14', 'Clothing')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try modifying tuple (will raise error)\n",
        "try:\n",
        "    valid_tuples[0][2] = 100\n",
        "except TypeError as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "id": "edIRpg_VfQ49",
        "outputId": "e9a465bc-517c-4271-bba4-d87c3628813d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'tuple' object does not support item assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Day 7: Data Analysis with Lists and Tuples\n",
        "- **Topics**: Sorting lists, filtering, list comprehensions.\n",
        "- **Task**: Generate insights (e.g., top 3 customers by total spend).\n",
        "- **Activity**:\n",
        "  - Create a list of (customer_name, total_amount) tuples.\n",
        "  - Sort and extract top 3 customers.\n"
      ],
      "metadata": {
        "id": "0945dymciGFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = read_csv('transactions.csv')\n",
        "valid_tuples, _ = clean_data_to_tuples(transactions)\n",
        "_ = [print(valid) for valid in valid_tuples]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-vnPw3njBpT",
        "outputId": "5915fea8-c539-4f42-8c93-1e1acf43dbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('1', 'John Doe', 50.25, '2025-06-01', 'Food')\n",
            "('2', 'Jane Smith', 120.5, '2025-06-02', 'Electronics')\n",
            "('3', 'Alice Johnson', 30.0, '2025-06-03', 'Clothing')\n",
            "('4', 'John Doe', 75.8, '2025-06-03', 'Food')\n",
            "('5', 'Bob Wilson', 200.0, '2025-06-04', 'Electronics')\n",
            "('6', 'Jane Smith', 15.99, '2025-06-05', 'Books')\n",
            "('7', 'Alice Johnson', 45.6, '2025-06-06', 'Clothing')\n",
            "('9', 'Emma Davis', 80.0, '2025-06-08', 'Electronics')\n",
            "('10', 'Bob Wilson', 25.5, '2025-06-09', 'Books')\n",
            "('11', 'Jane Smith', 60.75, '2025-06-10', 'Food')\n",
            "('13', 'Emma Davis', 90.2, '2025-06-12', 'Electronics')\n",
            "('14', 'John Doe', 35.4, '2025-06-13', 'Books')\n",
            "('15', 'Alice Johnson', 70.0, '2025-06-14', 'Clothing')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_customers(valid_tuples, n=3, ascending=True):\n",
        "    customer_totals = []\n",
        "    customers = []\n",
        "    for transaction in valid_tuples:\n",
        "        customer, amount = transaction[1], transaction[2]\n",
        "        if customer not in customers:\n",
        "            customers.append(customer)\n",
        "            customer_totals.append(amount)\n",
        "        else:\n",
        "            index = customers.index(customer)\n",
        "            customer_totals[index] += amount\n",
        "    customer_summary = list(zip(customers, customer_totals))\n",
        "    get_first_element_of_tuple = lambda x: x[1]\n",
        "    topN_customer = sorted(customer_summary, key=get_first_element_of_tuple, reverse=ascending)[:n]\n",
        "    return topN_customer[:n]\n",
        "### ascending: tăng dần\n",
        "top_customers = get_top_customers(valid_tuples, n=3, ascending=False)\n",
        "\n",
        "print(\"Top 3 customers by spend:\")\n",
        "for customer, total in top_customers:\n",
        "    print(f\"{customer}: ${total:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWhTxyxuiIyR",
        "outputId": "b9859eb3-03f7-48d0-b1c0-133a2f046384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 customers by spend:\n",
            "Alice Johnson: $145.60\n",
            "John Doe: $161.45\n",
            "Emma Davis: $170.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A lambda function** in Python is a small, anonymous (unnamed) function defined using the lambda keyword. It’s used for short, one-off operations, especially in places like key parameters for sorting or filtering.\n",
        "\n",
        "Syntax\n",
        "```python\n",
        "lambda arguments: expression\n",
        "```\n",
        "- Arguments: Inputs to the function (e.g., x).\n",
        "- Expression: A single expression evaluated and returned (e.g., x[1]).\n",
        "- Example: ```lambda x: x[1]``` takes a tuple x and returns its second element.\n",
        "##### Difference Between lambda and def Keyword\n",
        "lambda is concise but less powerful than def when handling complex logic. Let's take a look at short comparison between the two:\n",
        "\n",
        "| Feature           | `lambda` Function                    | Regular Function (`def`)               |\n",
        "| ----------------- | ------------------------------------ | -------------------------------------- |\n",
        "| **Definition**    | Single expression with `lambda`.     | Multiple lines of code.                |\n",
        "| **Name**          | Anonymous (or named if assigned).    | Must have a name.                      |\n",
        "| **Statements**    | Single expression only.              | Can include multiple statements.       |\n",
        "| **Documentation** | Cannot have a docstring.             | Can include docstrings.                |\n",
        "| **Reusability**   | Best for short, temporary functions. | Better for reusable and complex logic. |"
      ],
      "metadata": {
        "id": "Vgj2B3UsnhhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"====> Example 1: Sign of x\")\n",
        "sign = lambda x: \"Positive\" if x > 0 else \"Negative\" if x < 0 else \"Zero\"\n",
        "print(f\"Sign of 10 is {sign(10)}\")\n",
        "print(f\"Sign of -10 is {sign(-10)}\")\n",
        "print(f\"Sign of 0 is {sign(0)}\")\n",
        "print(\"====> Example 2: Uppercase\")\n",
        "upperString = lambda sttr: sttr.upper()\n",
        "print(f\"Uppercase of 'hello' is {upperString('hello')}\")\n",
        "print(\"====> Example 3: Relu Function\")\n",
        "Relu = lambda x: x if x > 0 else 0\n",
        "print(f\"Relu of 10 is {Relu(10)}\")\n",
        "print(f\"Relu of -10 is {Relu(-10)}\")\n",
        "print(\"====> Example 4: Multi statements\")\n",
        "calculate = lambda x, y, op: x + y if op == \"add\" else x - y if op == \"sub\" else \"Invalid operation\"\n",
        "print(f\"2 added 3 equal to {calculate(2,3,op='add')}\")\n",
        "print(f\"2 subtracted 3 equal to {calculate(2,3,op='sub')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D89iWmHrqeZh",
        "outputId": "ee186e22-d430-456a-dfca-e6f1cf9815ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====> Example 1: Sign of x\n",
            "Sign of 10 is Positive\n",
            "Sign of -10 is Negative\n",
            "Sign of 0 is Zero\n",
            "====> Example 2: Uppercase\n",
            "Uppercase of 'hello' is HELLO\n",
            "====> Example 3: Relu Function\n",
            "Relu of 10 is 10\n",
            "Relu of -10 is 0\n",
            "====> Example 4: Multi statements\n",
            "2 added 3 equal to 5\n",
            "2 subtracted 3 equal to -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Usage with filter/map function()\n",
        "[GeeksforGeeks - Python Lambda Functions](https://www.geeksforgeeks.org/python/python-lambda-anonymous-functions-filter-map-reduce/)"
      ],
      "metadata": {
        "id": "cRMfFLgGuxB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = [1, 2, 3, 4, 5, 6]\n",
        "isEven = lambda x: x % 2 == 0\n",
        "even = filter(isEven, n)\n",
        "print(f\"1. Filtering even number only from {n}:\\n>>> {list(even)}\")\n",
        "\n",
        "a = [1, 2, 3, 4]\n",
        "double = lambda x: x * 2\n",
        "b = map(double, a)\n",
        "print(f\"2. Map doubling each elements from {a}:\\n>>> {list(b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4591dEvZujkX",
        "outputId": "d9bd5d44-2a13-4aca-b858-f2e67a78fc6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Filtering even number only from [1, 2, 3, 4, 5, 6]:\n",
            ">>> [2, 4, 6]\n",
            "2. Map doubling each elements from [1, 2, 3, 4]:\n",
            ">>> [2, 4, 6, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Others: usage with reduce function()"
      ],
      "metadata": {
        "id": "DglGibyuvC0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "# Example: Find the product of all numbers in a list\n",
        "a = [1, 2, 3, 4]\n",
        "multi = lambda x, y: x * y\n",
        "b = reduce(multi, a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03e0Ca0jvKmy",
        "outputId": "7f7b9a5e-8e3b-45d1-d707-4ef2c98d9de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day 8: Error Handling and Robustness\n",
        "- **Topics**: Try-except, custom error messages.\n",
        "- **Task**: Add error handling for file operations and data parsing.\n",
        "- **Activity**:\n",
        "  - Handle missing file errors and malformed CSV rows.\n",
        "  - Test with a broken CSV file.\n",
        "  "
      ],
      "metadata": {
        "id": "1NmHU7IWv3qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data_to_tuples(transactions):\n",
        "    valid = []\n",
        "    invalid = []\n",
        "    for row in transactions:\n",
        "        try:\n",
        "            amount = float(row[2])\n",
        "            if amount >= 0:\n",
        "                valid.append((row[0], row[1], amount, row[3], row[4]))\n",
        "            else:\n",
        "                invalid.append(row)\n",
        "        ## Handle get value not as expected (i.e: String instead of numeric)\n",
        "        except ValueError:\n",
        "            invalid.append(row)\n",
        "    return valid, invalid\n",
        "\n",
        "def read_csv_safe(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            transactions = [line.strip().split(',') for line in file]\n",
        "            ### Handle case of empty file\n",
        "            if not transactions:\n",
        "                raise ValueError(\"Empty CSV file\")\n",
        "            return transactions\n",
        "    ## Handle missing file in case cannot open file with file_path as filled\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found\")\n",
        "        return []\n",
        "    ## Handle any errors while open for reading file\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return []\n",
        "transactions = read_csv_safe('transactions.csv')\n",
        "if transactions:\n",
        "    valid_tuples, invalid = clean_data_to_tuples(transactions[1:])  # Skip header\n",
        "    print(f\"Processed {len(valid_tuples)} valid transactions\")\n"
      ],
      "metadata": {
        "id": "wBfGb8cZv6To",
        "outputId": "8f0791e5-02cb-45f3-f7f4-ac4a249d2237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 13 valid transactions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day 9: Final Pipeline Integration\n",
        "- **Topics**: Combining functions, script structure.\n",
        "- **Task**: Build a complete pipeline script.\n",
        "- **Activity**:\n",
        "  - Integrate all functions into a single script.\n",
        "  - Output results to console and a summary file.\n",
        "\n",
        "- This diagrama visualize the workflow of this project in form of a data pipeline.\n",
        "<center>\n",
        " <img src='https://github.com/newfrogg/data_engineering/blob/review_python/images/data_pipeline.png?raw=1' width='250%' />\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "LQmWOushl_p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv_safe(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            transactions = [line.strip().split(',') for line in file]\n",
        "            if not transactions:\n",
        "                raise ValueError(\"Empty CSV file\")\n",
        "            return transactions\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return []\n",
        "def clean_data_to_tuples(transactions):\n",
        "    valid = []\n",
        "    invalid = []\n",
        "    for row in transactions:\n",
        "        try:\n",
        "            amount = float(row[2])\n",
        "            if amount >= 0:\n",
        "                valid.append((row[0], row[1], amount, row[3], row[4]))\n",
        "            else:\n",
        "                invalid.append(row)\n",
        "        except (ValueError, IndexError):\n",
        "            invalid.append(row)\n",
        "    return valid, invalid\n",
        "def calculate_totals(valid_tuples):\n",
        "    total_sales = sum(t[2] for t in valid_tuples)\n",
        "    return total_sales\n",
        "def get_top_customers(valid_tuples, n=3):\n",
        "    customer_totals = []\n",
        "    customers = []\n",
        "    for t in valid_tuples:\n",
        "        customer = t[1]\n",
        "        amount = t[2]\n",
        "        if customer not in customers:\n",
        "            customers.append(customer)\n",
        "            customer_totals.append(amount)\n",
        "        else:\n",
        "            index = customers.index(customer)\n",
        "            customer_totals[index] += amount\n",
        "    customer_summary = [(c, t) for c, t in zip(customers, customer_totals)]\n",
        "    customer_summary.sort(key=lambda x: x[1], reverse=True)\n",
        "    return customer_summary[:n]\n",
        "def write_summary(file_path, total_sales, top_customers, invalid_count):\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(f\"Transaction Summary\\n\")\n",
        "        file.write(f\"Total Sales: ${total_sales:.2f}\\n\")\n",
        "        file.write(f\"Invalid Transactions: {invalid_count}\\n\")\n",
        "        file.write(f\"Top {len(top_customers)} Customers:\\n\")\n",
        "        for customer, total in top_customers:\n",
        "            file.write(f\"{customer}: ${total:.2f}\\n\")\n",
        "# Main pipeline\n",
        "def run_pipeline(input_file, output_file):\n",
        "    transactions = read_csv_safe(input_file)\n",
        "    if not transactions:\n",
        "        return\n",
        "    valid_tuples, invalid = clean_data_to_tuples(transactions[1:])  # Skip header\n",
        "    total_sales = calculate_totals(valid_tuples)\n",
        "    top_customers = get_top_customers(valid_tuples)\n",
        "    write_summary(output_file, total_sales, top_customers, len(invalid))\n",
        "    print(f\"Pipeline complete. Summary written to {output_file}\")\n",
        "# Run\n",
        "run_pipeline('transactions.csv', 'summary.txt')\n",
        "!cat summary.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0Wb33CHmGU8",
        "outputId": "86c1d863-2aad-42f9-c4d4-5e687f38d227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline complete. Summary written to summary.txt\n",
            "Transaction Summary\n",
            "Total Sales: $899.99\n",
            "Invalid Transactions: 2\n",
            "Top 3 Customers:\n",
            "Bob Wilson: $225.50\n",
            "Jane Smith: $197.24\n",
            "Emma Davis: $170.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_tuples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSpcDROPmRxe",
        "outputId": "108846a7-3724-46c5-f999-0d406bec9336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', 'John Doe', 50.25, '2025-06-01', 'Food'), ('2', 'Jane Smith', 120.5, '2025-06-02', 'Electronics'), ('3', 'Alice Johnson', 30.0, '2025-06-03', 'Clothing'), ('4', 'John Doe', 75.8, '2025-06-03', 'Food'), ('5', 'Bob Wilson', 200.0, '2025-06-04', 'Electronics'), ('6', 'Jane Smith', 15.99, '2025-06-05', 'Books'), ('7', 'Alice Johnson', 45.6, '2025-06-06', 'Clothing'), ('9', 'Emma Davis', 80.0, '2025-06-08', 'Electronics'), ('10', 'Bob Wilson', 25.5, '2025-06-09', 'Books'), ('11', 'Jane Smith', 60.75, '2025-06-10', 'Food'), ('13', 'Emma Davis', 90.2, '2025-06-12', 'Electronics'), ('14', 'John Doe', 35.4, '2025-06-13', 'Books'), ('15', 'Alice Johnson', 70.0, '2025-06-14', 'Clothing')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day 10: Testing and Extensions\n",
        "- **Topics**: Testing code, debugging.\n",
        "- **Task**: Test the pipeline and add a new feature.\n",
        "- **Activity**:\n",
        "  - Test with different CSV files (e.g., missing columns, large data).\n",
        "  - Add a function to filter transactions by date range.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7VtSWXzXnIA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filer_cond = lambda t, start, end: start <= datetime.strptime(t[3], '%Y-%m-%d') <= end\n",
        "result = list(filter(lambda t: filer_cond(t, start, end), valid_tuples))\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPxYZk8Gn0k-",
        "outputId": "6f7024db-c04c-480f-f00f-0733598850d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', 'John Doe', 50.25, '2025-06-01', 'Food'), ('2', 'Jane Smith', 120.5, '2025-06-02', 'Electronics'), ('3', 'Alice Johnson', 30.0, '2025-06-03', 'Clothing'), ('4', 'John Doe', 75.8, '2025-06-03', 'Food'), ('5', 'Bob Wilson', 200.0, '2025-06-04', 'Electronics'), ('6', 'Jane Smith', 15.99, '2025-06-05', 'Books'), ('7', 'Alice Johnson', 45.6, '2025-06-06', 'Clothing'), ('9', 'Emma Davis', 80.0, '2025-06-08', 'Electronics'), ('10', 'Bob Wilson', 25.5, '2025-06-09', 'Books'), ('11', 'Jane Smith', 60.75, '2025-06-10', 'Food'), ('13', 'Emma Davis', 90.2, '2025-06-12', 'Electronics'), ('14', 'John Doe', 35.4, '2025-06-13', 'Books'), ('15', 'Alice Johnson', 70.0, '2025-06-14', 'Clothing')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cat transactions.csv | column -t -s \",\"\n",
        "from datetime import datetime\n",
        "\n",
        "def filter_by_date(valid_tuples, start_date, end_date):\n",
        "    filter_cond = lambda transact, start, end: start <= datetime.strptime(transact[3], '%Y-%m-%d') <= end\n",
        "    result = list(filter(lambda t: filter_cond(t, start, end), valid_tuples))\n",
        "\n",
        "    return result\n",
        "# Example usage in pipeline\n",
        "from datetime import datetime\n",
        "start = datetime(2025, 6, 4)\n",
        "end = datetime(2025, 6, 15)\n",
        "filtered_tuples = filter_by_date(valid_tuples, start, end)\n",
        "print(f\"Transactions from {start.date()} to {end.date()}: {len(filtered_tuples)}\")\n",
        "for t in filtered_tuples:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqsBumkHnKxU",
        "outputId": "3dc9b421-c1b3-49ff-d6a2-6ff8970e9890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transactions from 2025-06-04 to 2025-06-15: 9\n",
            "('5', 'Bob Wilson', 200.0, '2025-06-04', 'Electronics')\n",
            "('6', 'Jane Smith', 15.99, '2025-06-05', 'Books')\n",
            "('7', 'Alice Johnson', 45.6, '2025-06-06', 'Clothing')\n",
            "('9', 'Emma Davis', 80.0, '2025-06-08', 'Electronics')\n",
            "('10', 'Bob Wilson', 25.5, '2025-06-09', 'Books')\n",
            "('11', 'Jane Smith', 60.75, '2025-06-10', 'Food')\n",
            "('13', 'Emma Davis', 90.2, '2025-06-12', 'Electronics')\n",
            "('14', 'John Doe', 35.4, '2025-06-13', 'Books')\n",
            "('15', 'Alice Johnson', 70.0, '2025-06-14', 'Clothing')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Achievement & Insights\n",
        "\n",
        "### Deliverables\n",
        "- A complete Python script (`pipeline.py`) that runs the data pipeline.\n",
        "- A sample `transactions.csv` file.\n",
        "- A `summary.txt` file with analysis results.\n",
        "- (Optional) A short report (in comments or separate file) explaining the pipeline.\n",
        "\n",
        "### Learning Outcomes\n",
        "- **Python Basics**: Master variables, data types, conditionals, loops, functions.\n",
        "- **Lists and Tuples**: Understand mutable vs. immutable data structures.\n",
        "- **Data Engineering**: Learn data ingestion, cleaning, and pipeline design.\n",
        "- **Data Analysis**: Perform basic aggregations and generate insights.\n",
        "\n",
        "### Next Steps\n",
        "- Explore Python dictionaries for more complex aggregations.\n",
        "- Learn `pandas` for advanced data manipulation.\n",
        "- Add database integration (e.g., SQLite) for data engineering.\n",
        "\n"
      ],
      "metadata": {
        "id": "xkoHpXV-_ZuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQL Basics (deprecated)"
      ],
      "metadata": {
        "id": "sf6lN9_4ltQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext sql\n",
        "%timeit 2 + 2"
      ],
      "metadata": {
        "id": "fckmeGf1l6xA",
        "outputId": "c4675898-1ef0-40d7-ba89-c60b07fbb5a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sql extension is already loaded. To reload it, use:\n",
            "  %reload_ext sql\n",
            "9.05 ns ± 0.116 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnh5CT9XQ6HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit\n",
        "2 + 2"
      ],
      "metadata": {
        "id": "85zDpvY1m55F",
        "outputId": "977734eb-24bc-492f-e673-1c4f3ed0d32c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# If running in Google Colab, install PostgreSQL and restore the database\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Update package installer\n",
        "    !sudo apt-get update -qq > /dev/null 2>&1\n",
        "\n",
        "    # Install PostgreSQL\n",
        "    !sudo apt-get install postgresql -qq > /dev/null 2>&1\n",
        "\n",
        "    # Start PostgreSQL service (suppress output)\n",
        "    !sudo service postgresql start > /dev/null 2>&1\n",
        "\n",
        "    # Set password for the 'postgres' user to avoid authentication errors (suppress output)\n",
        "    !sudo -u postgres psql -c \"ALTER USER postgres WITH PASSWORD 'password';\" > /dev/null 2>&1\n",
        "\n",
        "    # Create the 'colab_db' database (suppress output)\n",
        "    !sudo -u postgres psql -c \"CREATE DATABASE contoso_100k;\" > /dev/null 2>&1\n",
        "\n",
        "    # Download the PostgreSQL .sql dump\n",
        "    !wget -q -O contoso_100k.sql https://github.com/lukebarousse/Int_SQL_Data_Analytics_Course/releases/download/v.0.0.0/contoso_100k.sql\n",
        "\n",
        "    # Restore the dump file into the PostgreSQL database (suppress output)\n",
        "    !sudo -u postgres psql contoso_100k < contoso_100k.sql > /dev/null 2>&1\n",
        "\n",
        "    # Shift libraries from ipython-sql to jupysql\n",
        "    !pip uninstall -y ipython-sql > /dev/null 2>&1\n",
        "    !pip install jupysql > /dev/null 2>&1\n",
        "\n",
        "# Load the sql extension for SQL magic\n",
        "%load_ext sql\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "%sql postgresql://postgres:password@localhost:5432/contoso_100k\n",
        "\n",
        "# Enable automatic conversion of SQL results to pandas DataFrames\n",
        "%config SqlMagic.autopandas = True\n",
        "\n",
        "# Disable named parameters for SQL magic\n",
        "%config SqlMagic.named_parameters = \"disabled\"\n",
        "\n",
        "# Display pandas number to two decimal places\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "metadata": {
        "id": "NRt2f43WnB0G",
        "outputId": "fc4b92fe-b2dc-4d3c-8fd3-44c0e6b684de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sql extension is already loaded. To reload it, use:\n",
            "  %reload_ext sql\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: UserWarning: Config option `named_parameters` not recognized by `SqlMagic`.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS50 SQL\n",
        "- This note is reference from [CS50’s Introduction to Databases with SQL](https://cs50.harvard.edu/sql)\n",
        "- Convert URL to Markdown using this tool: [url-to-markdown](https://jsongpt.com/converter/url-to-markdown)"
      ],
      "metadata": {
        "id": "IBATgsrIQ-a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Database ?\n",
        "A database is **a way of organizing data** such that you can perform four *operations* on it:\n",
        "- create\n",
        "- read\n",
        "- update\n",
        "- delete\n",
        "\n",
        "The choice of a DBMS would rest on factors like:\n",
        "- **Cost**: proprietary vs. free software,\n",
        "- **Amount of support**: free and open source software like MySQL, PostgreSQL and SQLite come with the downside of having to set up the database yourself,\n",
        "- **Weight**: more fully-featured systems like MySQL or PostgreSQL are heavier and require more computation to run than systems like SQLite.\n",
        "\n",
        "**Note:**\n",
        "> CS50 SQL start with SQLite and then move on to MySQL and PostgreSQL"
      ],
      "metadata": {
        "id": "Zuj-d_npaTCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQL\n",
        "SQL stands for **`Structured Query Language`**. It is a language used to interact with databases, via which you can create, read, update, and delete data in a database. Some important notes about SQL\n",
        "- it is **`structured`**, as we’ll see in this course,\n",
        "- it has some keywords that can be **`used to interact with the database`**, and\n",
        "- it is a query language — it can be used to **`ask questions of data`** inside a database.\n",
        "\n",
        "Note:\n",
        "- Get longlist.db from [here](https://cdn.cs50.net/sql/2025/spring/sections/0/)\n",
        "\n",
        "- Execute sql script in sqlite [here](https://database.guide/5-ways-to-run-sql-script-from-file-sqlite)"
      ],
      "metadata": {
        "id": "8DYc0-GebPo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query\n",
        "\n",
        "- [**Note**](https://cs50.harvard.edu/sql/notes/0/), [**Lecture**](https://cdn.cs50.net/sql/2023/x/lectures/0/lecture0.pdf)\n",
        "- [SQL script](./cs50_sql/scripts/00.long_list_sample.sql)\n",
        "- **Longlist database**: a database of books that have been longlisted for the International Booker Prize. Each year, there are 13 books on the longlist and our database contains 5 years’ worth of such longlists."
      ],
      "metadata": {
        "id": "VfbYyFHSRY3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem set 0\n",
        "\n",
        "### **Cyberspace**\n",
        "Welcome to Cyberspace! Cyberchase is an animated, educational kid’s television series, aired by the United States’ Public Broadcasting Service (PBS) since 2002. Originally designed to “*show kids that math is everywhere and everyone can be good at it,*” the world of Cyberchase centers on Jackie, Matt, and Inez as they team up with Digit—a “cybird”—to stop Hacker from taking over Cyberspace and infecting Motherboard. **Along the way, the quartet learn math, science, and problem-solving skills to thwart Hacker in his attempts**.\n",
        "\n",
        "In a database called cyberchase.db, using a table called episodes, chase answers to PBS’s questions about Cyberchase’s episodes thus far.\n",
        "\n",
        "### **Normals**\n",
        "How do we know whether ocean temperatures are lower or higher than “normal”? What’s a “normal” temperature? Turns out that scientists have developed a metric called a “Climate Normal.” A Climate Normal characterizes aspects of earth’s climate—its long-term weather—over a span of 30 years. One important metric is ocean temperature.\n",
        "\n",
        "In a database called normals.db, using a table called normals, explore some of the most recent Climate Normal data to understand what makes a normal ocean temperature around the world.\n",
        "\n",
        "Visualizer database:\n",
        "![normal database](https://cs50.harvard.edu/sql/psets/0/normals/normals.jpg)\n",
        "\n",
        "In normals.db you’ll find a single table of coordinates, normals. In the normals table, you’ll find the following columns:\n",
        "\n",
        "- **id**, which uniquely identifies each row (coordinate) in the table\n",
        "- **latitude**, which is the degree of latitude (expressed in decimal format) for the coordinate\n",
        "- **longitude**, which is the degree of longitude (expressed in decimal format) for the coordinate\n",
        "- **0m**, which is the normal ocean surface temperature (i.e., the normal temperature at 0 meters of depth), in degrees Celsius, at the coordinate\n",
        "- **5m**, which is the normal ocean temperature at 5 meters of depth, in degrees Celsius, at the coordinate\n",
        "- **10m**, which is the normal ocean temperature at 10 meters of depth, in degrees Celsius, at the coordinate\n",
        "\n",
        "### **Players**\n",
        "**Baseball** is a popular sport in which two teams of 9 players take turns batting (hitting a ball) and fielding (catching and throwing hit balls). Points (“runs”) are scored when a hitting team’s player hits a ball and eventually touches all bases before the fielding team’s players have the chance to get them “out.” Baseball is arguably most popular in the United States and Canada, where the MLB (Major League Baseball) has served as the professional association for players since 1876.\n",
        "\n",
        "In a database called players.db, using a table called players, answer questions about MLB players who’ve played from 1871 to 2023.\n",
        "\n",
        "In **`players.db`** you’ll find a single table, players. In the players table, you’ll find the following columns:\n",
        "\n",
        "- **id**, which uniquely identifies each row (player) in the table\n",
        "- **first_name**, which is the first name of the player\n",
        "- **last_name**, which is the last name of the player\n",
        "- **bats**, which is the side (R for right or L for left) the player bats on\n",
        "- **throws**, which is the hand (R for right or L for left) the player throws with\n",
        "- **weight**, which is the player’s weight in pounds\n",
        "- **height**, which is the player’s height in inches\n",
        "- **debut**, which is the date (expressed as YYYY-MM-DD) the player began their career in the MLB\n",
        "- **final_game**, which is the date (expressed as YYYY-MM-DD) the player played their last game in the MLB\n",
        "- **birth_year**, which is the year the player was born\n",
        "- **birth_month**, which is the month (expressed as an integer) the player was born\n",
        "- **birth_day**, which is the day the player was born\n",
        "- **birth_city**, which is the city in which the player was born\n",
        "- **birth_state**, which is the state in which the player was born\n",
        "- **birth_country**, which is the country in which the player was born"
      ],
      "metadata": {
        "id": "Xp32ApAOXKu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schema**\n",
        "\n",
        "Each database has some “schema”—the tables and columns into which the data is organized. In **`cyberchase.db`** you’ll find a single table, episodes. In the episodes table, you’ll find the following column.\n",
        "\n",
        "\n",
        "![Database Stucture](https://github.com/newfrogg/data_engineering/blob/sql_intro/images/cyberchase_db_structure.png?raw=true)\n",
        "\n",
        "This image above illustrates the stucture of this database:\n",
        "- 1 schema (episodes) with relating columns:\n",
        "- `id`, which uniquely identifies each row (episode) in the table\n",
        "- `season`, which is the season number in which the episode aired\n",
        "- `episode_in_season`, which is the episode’s number within its given season\n",
        "- `title`, which is the episode’s title\n",
        "- `topic`, which identifies the ideas the episode aimed to teach\n",
        "- `air_date`, which is the date (expressed as YYYY-MM-DD) on which the episode “aired” (i.e., was published)\n",
        "- `production_code`, which is the unique ID used by PBS to refer to each episode internally"
      ],
      "metadata": {
        "id": "f8Dp0nmxYDSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0ERLyVPUSvks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relating\n",
        "\n",
        "Databases can have **multiple tables**. Last class, we saw a database of books longlisted, or nominated, for the International Booker Prize. We will now see that database has many different tables inside it — for books, authors, publishers and so on.\n",
        "\n",
        "Some possible ways to organize books and authors are:\n",
        "- the honor system: the **`first row in the authors table will always correspond to the first row in the books table`**. The problem with this system is that one may make a mistake (add a book but forget to add its corresponding author, or vice versa). Also, an author may have written more than one book or a book may be co-written by multiple authors.\n",
        "\n",
        "- going back to a one-table approach: **`This approach could result in redundancy (duplication of data) if one author writes multiple books or if a book is co-written by multiple authors.`** Below is a snapshot of the one-table approach with some redundant data.\n",
        "\n",
        "**One-table approach: author with multiple books**\n",
        "\n",
        "After considering these ideas, it seems like having two different tables is the most efficient approach. Let us look at some different ways in which tables can be related to each other in relational databases.\n",
        "\n"
      ],
      "metadata": {
        "id": "kn53eyiSJtIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity Relationship Diagrams\n",
        "We just described one-to-one, one-to-many and many-to-many relationships between tables in a database. It is possible to visualize such relationships using an entity relationship (ER) diagram.\n",
        "\n",
        "\n",
        "![ER diagram](https://github.com/newfrogg/data_engineering/blob/sql_intro/images/longlist_ERdiagram.png?raw=true)"
      ],
      "metadata": {
        "id": "PeYhsqHJKTHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convetion: Zero, One, Many ?**\n",
        "\n",
        "Each line is this diagram is in crow’s foot notation.\n",
        "- The first line with a circle looks like a **`0 marked on the line`**. This line indicates that there are no relations.\n",
        "- The second line with a perpendicular line looks like a **`1 marked on the line`**. An entity with this arrow has to have at least one row that relates to it in the other table.\n",
        "- The third line, which looks like a crow’s foot, has **`many branches`**. This line means that the entity is related to many rows from another table.\n",
        "\n",
        "On observing the lines connecting the Book and Translator entities, we can say that books don’t need to have a translator. They could have zero to many translators. However, ***a translator in the database translates at least one book, and possibly many.***"
      ],
      "metadata": {
        "id": "oaxRegrML8wH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **If we have some database, how do we know the relationships among the entities stored inside of it?**\n",
        "\n",
        "The exact relationships between entities are really ***up to the designer of the database***. For example, whether each author can write only one book or multiple books is a decision to be made while designing the database. An ER diagram can be thought of as a tool to communicate these decisions to someone who wants to understand the database and the relationships between its entities.\n",
        "\n",
        "> **Once we know that a relationship exists between certain entities, how do we implement that in our database?**\n",
        "\n",
        "We will shortly see how we can use ***keys*** in SQL to relate tables to one another."
      ],
      "metadata": {
        "id": "D24nFT24MxoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keys\n",
        "#### Primary Keys\n",
        "- In the case of books, every book has a unique identifier called an ISBN. In other words, **`if you search for a book by its ISBN, only one book will be found`**. In database terms, the ISBN is a primary key — an identifier that is unique for every item in a table.\n",
        "\n",
        "- Inspired by this idea of an ISBN, we can imagine assigning unique IDs to our publishers, authors and translators! **`Each of these IDs would be the primary key of the table it belongs to`**.\n",
        "\n",
        "#### Foreign Keys\n",
        "Keys also help relate tables in SQL. A foreign key is a primary key taken from a different table. By referencing the primary key of a different table, it helps relate the tables by forming a link between them.\n",
        "\n",
        "> **\"Relating the books and ratings tables using foreign key\"**\n",
        "\n",
        "Notice how the primary key of the books table is now a column in the ratings table. This helps form a one-to-many relationship between the two tables — a book with a title (found in the books table) can have multiple ratings (found in the ratings table).\n",
        "\n",
        "The ISBN, as we can see, is a long identifier. If each character occupied a byte of memory, storing a single ISBN (including the dashes) would take 17 bytes of memory, which is a lot!\n",
        "\n",
        "Thankfully, we don’t necessarily have to use the ISBN as a primary key. We can just construct our own using numbers like 1, 2, 3… and so on as long as each book has a unique number to identify it.\n",
        "\n",
        "![Foreign keys](https://cs50.harvard.edu/sql/notes/1/images/p40.jpg)\n",
        "\n"
      ],
      "metadata": {
        "id": "mf42AqgeNKbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Can the IDs of the author and the book be the same? For example, if author_id is 1 and book_id is also 1 in the authored table, will there be a mix-up?**\n",
        "\n",
        "Tables like authored are called “joint” or “junction” tables. In such tables, we usually know which primary key is referenced by which column. In this case, since we know that the first column contains the primary key of authors only and the second column similarly contains the primary key of books only, it would be okay even if the values matched!\n",
        "\n",
        "> **If we have a lot of joint tables like this, wouldn’t that take up too much space?**\n",
        "\n",
        "Yes, there is a trade-off here. Tables like these occupy more space but they also enable us to have many-to-many relationships without redundancies, like we saw earlier.\n",
        "\n",
        "> **On changing the ID of a book or author, does the ID get updated in the other tables as well?**\n",
        "\n",
        "An updated ID still needs to be unique. Given that, IDs are often abstracted away and we rarely change them."
      ],
      "metadata": {
        "id": "zdyvoJBKOIHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subqueries\n",
        "\n",
        "*   A subquery is a query inside another query. These are also called nested queries.\n",
        "*   Consider this example for a one-to-many relationship. In the `books` table, we have an ID to indicate the publisher, which is a foreign key taken from the `publishers` table. To find out the books published by Fitzcarraldo Editions, we would need two queries — **one to find out the `publisher_id` of Fitzcarraldo Editions from the `publishers` table and the second**, to use this `publisher_id` to find all the books published by Fitzcarraldo Editions. These two queries can be combined into one using the idea of a subquery.\n",
        "\n",
        "```sql\n",
        "SELECT \"title\"\n",
        "FROM \"books\"\n",
        "WHERE \"publisher_id\" = (\n",
        "    SELECT \"id\"\n",
        "    FROM \"publishers\"\n",
        "    WHERE \"publisher\" = 'Fitzcarraldo Editions'\n",
        ");\n",
        "```\n",
        "\n",
        "Notice that:\n",
        "\n",
        "*   The subquery is in parentheses. The query that is furthest inside parantheses will be run first, followed by outer queries.\n",
        "*   The inner query is indented. This is done as per style conventions for subqueries, to increase readability.\n",
        "*   To find all the ratings for the book In Memory of Memory"
      ],
      "metadata": {
        "id": "o-nS0PpuIDIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IN\n",
        "\n",
        "*   This keyword is used to check whether the desired value is _in_ a given list or set of values.\n",
        "*   The relationship between authors and books is ***many-to-many***. This means that it is possible a given author has written more than one book. To find the names of all books in the database written by Fernanda Melchor, we would use the `IN` keyword as follows.\n",
        "\n",
        "```sql\n",
        "SELECT \"title\"\n",
        "FROM \"books\"\n",
        "WHERE \"id\" IN (\n",
        "    SELECT \"book_id\"\n",
        "    FROM \"authored\"\n",
        "    WHERE \"author_id\" = (\n",
        "        SELECT \"id\"\n",
        "        FROM \"authors\"\n",
        "        WHERE \"name\" = 'Fernanda Melchor'\n",
        "    )\n",
        ");\n",
        "```\n",
        "\n",
        "![Foreign keys](https://cs50.harvard.edu/sql/notes/1/images/p40.jpg)\n",
        "\n",
        "Note that the innermost query uses `=` and not the `IN` operator. This is because we expect to find just one author named Fernanda Melchor.\n",
        "\n",
        "The **IN** keyword describes the relation: *one-to-many, one author -> many books, one author_id -> book_id (final id)*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VMJRqx0gIXu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **What if the value of an inner query is not found?**\n",
        "\n",
        "*   In this case, the inner query would return nothing, prompting the outer query to also return nothing. The outer query is thus dependent on the results of the inner query.\n",
        "\n",
        "> **Is it necessary to use four spaces to indent a subquery?**\n",
        "\n",
        "*   No. The number of spaces used to indent a subquery can vary, as can the length of each line in the query. But the central idea behind breaking up queries and indenting subqueries is to make them readable.\n",
        "\n",
        "> **How can we implement a many-to-one relationship between tables?**\n",
        "\n",
        "*   Consider the situation wherein a book is co-written by multiple authors. We would have an `authored` table with multiple entries for the same book ID. Each of these entries would have a different author ID. It is worth noting that foreign key values can be repeated within a table, but primary key values are always unique."
      ],
      "metadata": {
        "id": "DKyeKxW6JqI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JOIN\n",
        "\n",
        "*   This keyword allows us to combine two or more tables together.\n",
        "*   To understand how `JOIN` works, **consider a database of sea lions and their migration patterns**. Here is a snapshot of the database.\n",
        "*   To find out how far the sea lion Spot travelled, or answer similar questions about each sea lion, we could use nested queries. Alternately, we could join the tables `sea lions` and `migrations` together such that each sea lion also has its corresponding information as an extension of the same row.\n",
        "*   ***We can join the tables on the sea lion ID (the common factor between the two tables) to ensure that the correct rows are lined up against each other***.\n",
        "*   Before testing this out, make sure to exit `longlist.db` using the `.quit` SQLite command. Then, open up `sea_lions.db`.\n",
        "*   Sea Lions database:\n",
        "![database](https://cs50.harvard.edu/sql/notes/1/images/p69.jpg)\n",
        "*   To join the tables\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM \"sea_lions\"\n",
        "JOIN \"migrations\" ON \"migrations\".\"id\" = \"sea_lions\".\"id\";\n",
        "```\n",
        "Getting **sea_lions.db** from [here](https://cdn.cs50.net/sql/2023/x/lectures/1/src1/)\n",
        "\n",
        "Notice that:\n",
        "- ***The ON keyword is used to specify which values match between the tables being joined. It is not possible to join tables without matching values.***\n",
        "- If there are any IDs in one table not present in the other, this row will not be present in the joined table. This kind of join is called an INNER JOIN.\n",
        "- Some other ways of joining tables that allow us to retain certain unmatched IDs are `LEFT JOIN`, `RIGHT JOIN` and `FULL JOIN`. Each of these is a kind of `OUTER JOIN`.\n",
        "- A `LEFT JOIN` prioritizes the data in the left (or first) table.\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM \"sea_lions\"\n",
        "LEFT JOIN \"migrations\" ON \"migrations\".\"id\" = \"sea_lions\".\"id\";\n",
        "```\n",
        "\n",
        "This query would retain all sea lion data from the `sea_lions` table — the left one. ***Some rows in the joined table could be partially blank***. This would happen if the right table didn’t have data for a particular ID.\n",
        "\n",
        "*   Similarly, a `RIGHT JOIN` retains all the rows from the right (or second) table. A `FULL JOIN` allows us to see the entirety of all tables.\n",
        "*   As we can observe, an `OUTER JOIN` could lead to empty or `NULL` values in the joined table.\n",
        "*   Both tables in the sea lions database have the column `id`. Since the value on which we are joining the tables has the same column name in both tables, we can actually omit the `ON` section of the query while joining.\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM \"sea_lions\"\n",
        "NATURAL JOIN \"migrations\";\n",
        "```\n",
        "\n",
        "Notice that the result does not have a duplicate `id` column in this case. Also, this join works similarly to an `INNER JOIN`.\n",
        "\n"
      ],
      "metadata": {
        "id": "OHFKCV5rKdKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **In the sea lions database, how are the IDs created? Do they come from the `sea_lions` table or the `migrations` table?**\n",
        "\n",
        "*   The ID of each sea lion likely came from researchers tracking the migration patterns of these sea lions. That is to say, the IDs were not generated in either of the tables, but were assigned at the source of the data itself.\n",
        "\n",
        "> **If we are trying to join three tables, how can we know which the left or right tables are?**\n",
        "\n",
        "*   For each `JOIN` statement, the first table before the keyword is the left one. The one that is involved in the `JOIN` keyword is the right table.\n",
        "\n",
        "> **When we join tables, does the resulting joined table get saved? Can we reference it later without joining again?**\n",
        "\n",
        "*   In the way that we are using `JOIN`, the result is a temporary table or a result set. It can be used for the duration of the query.\n",
        "\n",
        "> **There’s many different kinds of `JOIN`. Is there a default one we should use?**\n",
        "\n",
        "*   The simplest kind — just `JOIN` — is actually an `INNER JOIN` and that’s the default for SQL."
      ],
      "metadata": {
        "id": "8OAlQqB3MoJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sets\n",
        "*   New version of Long list Database:\n",
        "![Long list version 2 Database](https://github.com/newfrogg/data_engineering/blob/sql_intro/images/longlist_v2_db_structure.png?raw=true)\n",
        "*   Let’s take another example. In our database of books, we have authors and translators. A person could be either an author or a translator. **If the two sets have an intersection, it is also a possible that a person could be both an author and a translator of books.** We can use the `INTERSECT` operator to find this set.\n",
        "\n",
        "```sql\n",
        "SELECT \"name\" FROM \"translators\"\n",
        "INTERSECT\n",
        "SELECT \"name\" FROM \"authors\";\n",
        "```\n",
        "\n",
        "*   If a person is either an author or a translator, or both, they belong to the union of the two sets. In other words, this set is formed by **combining the author and translator sets**.\n",
        "\n",
        "```sql\n",
        "SELECT \"name\" FROM \"translators\"\n",
        "UNION\n",
        "SELECT \"name\" FROM \"authors\";\n",
        "```\n",
        "\n",
        "Notice that every author and every translator is included in this result set, but only once!\n",
        "\n",
        "*   A minor adjustment to the previous query gives us the profession of the person in the result set, based on whether they are an author or a translator.\n",
        "\n",
        "> **Point out one person is author or translator**\n",
        "\n",
        "```sql\n",
        "SELECT 'author' AS \"profession\", \"name\"\n",
        "FROM \"authors\"\n",
        "UNION\n",
        "SELECT 'translator' AS \"profession\", \"name\"\n",
        "FROM \"translators\";\n",
        "```\n",
        "\n",
        "*   Everyone who is an author and _only_ an author is included in the following set. The `EXCEPT` keyword can be used to find such a set. In other words, the set of translators is subtracted from the set of authors to form this one.\n",
        "\n",
        "```sql\n",
        "SELECT \"name\" FROM \"authors\"\n",
        "EXCEPT\n",
        "SELECT \"name\" FROM \"translators\";\n",
        "```\n",
        "\n",
        "These operators could be useful to answer many different questions. For example, we can find the books that Sophie Hughes and Margaret Jull Costa have **translated together**.\n",
        "```sql\n",
        "SELECT \"book_id\" FROM \"translated\"\n",
        "WHERE \"translator_id\" = (\n",
        "    SELECT \"id\" from \"translators\"\n",
        "    WHERE \"name\" = 'Sophie Hughes'\n",
        ")\n",
        "INTERSECT\n",
        "SELECT \"book_id\" FROM \"translated\"\n",
        "WHERE \"translator_id\" = (\n",
        "    SELECT \"id\" from \"translators\"\n",
        "    WHERE \"name\" = 'Margaret Jull Costa'\n",
        ");\n",
        "```\n",
        "\n",
        "> **Could we use `INTERSECT`, `UNION` etc. to perform operations on 3-4 sets?**\n",
        "\n",
        "*   Yes, absolutely. To intersect 3 sets, we would have to use the `INTERSECT` operator twice. An important note — we have to make sure to have the same number and same types of columns in the sets to be combined using `INTERSECT`, `UNION` etc.\n"
      ],
      "metadata": {
        "id": "LLYTDDKCUo3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Groups\n",
        "\n",
        "\n",
        "*   Consider the `ratings` table. For each book, we want to find the average rating of the book. To do this, we would first need to **group ratings together by book** and then average the ratings out for each book (each group).\n",
        "\n",
        "```sql\n",
        "SELECT \"book_id\", AVG(\"rating\") AS \"average rating\"\n",
        "FROM \"ratings\"\n",
        "GROUP BY \"book_id\";\n",
        "```\n",
        "\n",
        "***In this query, the `GROUP BY` keyword was used to create groups for each book and then collapse the ratings of the group into an average rating!***\n",
        "\n",
        "*   Now, we only want to see the books that ***are well-rated, with an average rating of over 4.***\n",
        "\n",
        "```sql\n",
        "SELECT \"book_id\", ROUND(AVG(\"rating\"), 2) AS \"average rating\"\n",
        "FROM \"ratings\"\n",
        "GROUP BY \"book_id\"\n",
        "HAVING \"average rating\" > 4.0;\n",
        "```\n",
        "\n",
        "Note that the **`HAVING` keyword is used here to specify a condition for the groups**, instead of `WHERE` (which can only be used to specify conditions for individual rows).\n",
        "\n",
        "### Questions\n",
        "\n",
        "> Is it possible to see the number of ratings given to each book?\n",
        "\n",
        "*   Yes, this would require a slight modification with the use of the `COUNT` keyword.\n",
        "\n",
        "```sql\n",
        "SELECT \"book_id\", COUNT(\"rating\")\n",
        "FROM \"ratings\"\n",
        "GROUP BY \"book_id\";\n",
        "```\n",
        "\n",
        "> Is it also possible to sort the data obtained here?\n",
        "\n",
        "*   Yes, it is. Say we wanted to find the average ratings per well-rated book, ordered in descending order.\n",
        "\n",
        "```sql\n",
        "SELECT \"book_id\", ROUND(AVG(\"rating\"), 2) AS \"average rating\"\n",
        "FROM \"ratings\"\n",
        "GROUP BY \"book_id\"\n",
        "HAVING \"average rating\" > 4.0\n",
        "ORDER BY \"average rating\" DESC;\n",
        "```"
      ],
      "metadata": {
        "id": "7O-AMBjEWo5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem set 1\n",
        "### **Packages**\n",
        "\n",
        "You are a **mail clerk for the city of Boston** and, as such, you ***oversee*** the delivery of mail across the city. For the most part, all packages sent are eventually delivered. Except, every once in while, a mystery falls into your lap: a missing package! For each customer that comes to you with a report of a missing package, your job is to determine:\n",
        "\n",
        "- The current address (or location!) of their missing package\n",
        "- The type of address or location (e.g. residential, business, etc.)\n",
        "- The contents of the package\n",
        "\n",
        "All you know is what the customers themselves will tell you. To solve each mystery, you’ll need to use the mail delivery service’s database, **`packages.db`**, which **contains data on the transit of packages around the city**. Using just the information in the database, your task is to help each customer find their missing package.\n",
        "\n",
        "**Schema**\n",
        "\n",
        "**`packages.db`** represents all recent package deliveries in the city of Boston. To do so, packages.db represents the following entities:\n",
        "\n",
        "- Drivers, who are the people that deliver packages\n",
        "- The packages themselves\n",
        "- Addresses, such as 1234 Main Street\n",
        "- Scans of packages, which represent confirmations a delivery driver picked up or dropped off a given package.\n",
        "\n",
        "These entities are related per the entity relationship (ER) diagram below:\n",
        "\n",
        "![Package ERD](https://github.com/newfrogg/data_engineering/blob/sql_intro/images/package_ERD.png?raw=true)\n",
        "\n",
        "**addresses table**\n",
        "The addresses table contains the following columns:\n",
        "- id, which is the ID of the address\n",
        "- address, which is the street address itself (i.e., 7660 Sharon Street)\n",
        "- type, which is the type of address (i.e., residential, commercial, etc.)\n",
        "\n",
        "**drivers table**\n",
        "The drivers table contains the following columns:\n",
        "- id, which is the ID of the driver\n",
        "- name, which is the first name of the driver\n",
        "\n",
        "**packages table**\n",
        "The packages table contains the following columns:\n",
        "- id, which is the ID of the package\n",
        "- contents, which contains the contents of the package\n",
        "- from_address_id, which is the ID of the address from which the package was sent\n",
        "- to_address_id, which is the ID of the address to which the package was sent. It’s not necessarily where it ended up!\n",
        "\n",
        "**scans table**\n",
        "The scans table contains the following columns:\n",
        "- id, which is the ID of the scan\n",
        "- driver_id, which is the ID of the driver who created the scan\n",
        "- package_id, which is the ID of the package scanned\n",
        "- address_id, which is the ID of the address where the package was scanned\n",
        "- action, which indicates whether the package was picked up (“Pick”) or dropped off (“Drop”)\n",
        "- timestamp, which is the day and time at which the package was scanned\n",
        "\n",
        "There are 3 lost packages with clues to find them. The detail answers are in [script/01.packages.sql](https://github.com/newfrogg/data_engineering/blob/sql_intro/cs50_sql/scripts/01.packages.sql)\n",
        "\n",
        "**The Lost Letter:** Clerk, my name’s Anneke. I live over at 900 Somerville Avenue. Not long ago, I sent out a special letter. It’s meant for my friend Varsha. She’s starting a new chapter of her life at 2 Finnegan Street, uptown. (That address, let me tell you: it was a bit tricky to get right the first time.) The letter is a congratulatory note—a cheery little paper hug from me to her, to celebrate this big move of hers. Can you check if it’s made its way to her yet?\n",
        "\n",
        "_At what type of address did the Lost Letter end up?: Residential_\n",
        "\n",
        "_At what address did the Lost Letter end up?: 2 Finnegan Street_\n",
        "\n",
        "**The Devious Delivery:** Good day to you, deliverer of the mail. You might remember that not too long ago I made my way over from the town of Fiftyville. I gave a certain box into your reliable hands and asked you to keep things low. My associate has been expecting the package for a while now. And yet, it appears to have grown wings and flown away. Ha! Any chance you could help clarify this mystery? Afraid there’s no “From” address. It’s the kind of parcel that would add a bit more… quack to someone’s bath times, if you catch my drift.\n",
        "\n",
        "_At what type of address did the Devious Delivery end up?: Residential_\n",
        "\n",
        "_What were the contents of the Devious Delivery?: Duck debugger_\n",
        "\n",
        "**The Forgotten Gift:** Oh, excuse me, Clerk. I had sent a mystery gift, you see, to my wonderful granddaughter, off at 728 Maple Place. That was about two weeks ago. Now the delivery date has passed by seven whole days and I hear she still waits, her hands empty and heart filled with anticipation. I’m a bit worried wondering where my package has gone. I cannot for the life of me remember what’s inside, but I do know it’s filled to the brim with my love for her. Can we possibly track it down so it can fill her day with joy? I did send it from my home at 109 Tileston Street.\n",
        "\n",
        "_What are the contents of the Forgotten Gift?: Chalk_\n",
        "\n",
        "_Who has the Forgotten Gift?: Mikel_"
      ],
      "metadata": {
        "id": "47IcYKV4t2ih"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}